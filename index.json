[{"authors":["admin"],"categories":null,"content":"I am an Assistant Professor (RTDA) in Statistics at the MEMOTEF Department of Sapienza University of Rome (IT). I also hold joint positions as Visiting Faculty Researcher at Google and as Visiting Researcher at the MRC - Biostatistics Unit, University of Cambridge (UK), where I was a PostDoc in 2020 \u0026mdash; 2021.\nI collaborate with various academic and nonacademic research institutions worldwide, some of which I visited during my PhD. Active collaborations include University of Cambridge, National University of Singapore and University of Toronto, where I am also part of the IAI Lab. In Italy, I am leading some work in collaboration with ISTAT (on accuracy estimation of multisource official statistics), NADO Italia (on doping detection), and FAO (on quantifying the impact of disasters on economic loss). I am in the editorial board of YoungStatS, the blog of Young Statisticians Europe (YSE), serve as as associate editor of Trials, and curate the Women in Statistics and Data Science Twitter account. I am also a member of IMS, ACM, ISCB, IBS, SIS and YoungSIS, among others.\nI love stats and I embrace diverse, and perhaps too many to mention, therein areas. My main attention so far has been devoted to sequential decision-making problems, intersecting areas of statistical inference, Bayesian statistics, reinforcement learning (RL) \u0026amp; multi-armed bandits (MABs), and adaptive design of experiments, main focus of my PhD thesis. More recently, I became interested in copula models for representing data dependencies, and how these can be used for improving statistical problems such as estimating highest density regions or anomaly detection. Uncertainty quantification, especially conformal prediction, is a new door I am opening, with excitement, and shall constitute one of my future research lines.\nMy research is inspired by the numerous challenges arising in real-life applications and directed towards providing concrete benefits in these areas. I strongly believe that the theoretical and methodological progress should go along with the concrete real-world needs, and be not simply good, but also good for something.\n","date":1644537600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1644537600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://nina-dl.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an Assistant Professor (RTDA) in Statistics at the MEMOTEF Department of Sapienza University of Rome (IT). I also hold joint positions as Visiting Faculty Researcher at Google and as Visiting Researcher at the MRC - Biostatistics Unit, University of Cambridge (UK), where I was a PostDoc in 2020 \u0026mdash; 2021.\nI collaborate with various academic and nonacademic research institutions worldwide, some of which I visited during my PhD. Active collaborations include University of Cambridge, National University of Singapore and University of Toronto, where I am also part of the IAI Lab.","tags":null,"title":"Nina Deliu","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://nina-dl.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://nina-dl.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://nina-dl.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"e9c8298a2d61d01cfcd427699e57e570","permalink":"https://nina-dl.github.io/research_old/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/research_old/example/","section":"research_old","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"research_old"},{"authors":["Nina Deliu","Sofia S Villar"],"categories":null,"content":"","date":1746835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746835200,"objectID":"be3af6d408ba31fe74620398fc422dd5","permalink":"https://nina-dl.github.io/publication/2025rptestbiometrics/","publishdate":"2025-05-10T00:00:00Z","relpermalink":"/publication/2025rptestbiometrics/","section":"publication","summary":"It is now commonly known that using optimal response-adaptive designs for data collection offers great potential in terms of optimizing expected outcomes, but poses multiple challenges for inferential goals. In many settings, such as phase-II or confirmatory clinical trials, a main barrier to their practical use is the lack of type-I error guarantees and/or power efficiency, especially in finite samples. This work addresses this gap. Specifically, focusing on a novel test statistic defined on the randomization probabilities of the (randomized) adaptive design, we derive its finite-sample and asymptotic guarantees. Further theoretical properties are evaluated under a Bayesian response-adaptive design, which is commonly used both in clinical trial applications and beyond (e.g., recommendation systems or mobile health). The frequentist error control advantages of the proposed approach–also able to preserve expected outcome optimalities–are illustrated in a real-world phase-II oncology trial and in simulation experiments.","tags":["Hypothesis testing","Response-adaptive designs","Thompson sampling","Multi-armed bandits"],"title":"On the finite-sample and asymptotic error control of a randomization-probability test for response-adaptive clinical trials","type":"publication"},{"authors":["Xueqing Liu","Nina Deliu","Tanujit Chakraborty","Lauren Bell","Bibhas Chakraborty"],"categories":null,"content":"","date":1741564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741564800,"objectID":"b572af36dfec5c5b7cc972ae0c63395f","permalink":"https://nina-dl.github.io/publication/2025zerots_aoas/","publishdate":"2025-03-10T00:00:00Z","relpermalink":"/publication/2025zerots_aoas/","section":"publication","summary":"Mobile health (mHealth) interventions often aim to improve distal out- comes, such as clinical conditions, by optimizing proximal outcomes through just-in-time adaptive interventions. Contextual bandits provide a suitable framework for customizing such interventions according to individual time- varying contexts. However, unique challenges, such as modeling count out- comes within bandit frameworks, have hindered the widespread application of contextual bandits to mHealth studies. The current work addresses this challenge by leveraging count data models into online decision-making ap- proaches. Specifically, we combine four common offline count data models (Poisson, negative binomial, zero-inflated Poisson, and zero-inflated negative binomial regressions) with Thompson sampling, a popular contextual ban- dit algorithm. The proposed algorithms are motivated by and evaluated on a real dataset from the Drink Less trial, where they are shown to improve user engagement with the mHealth platform. The proposed methods are further evaluated on simulated data, achieving improvement in maximizing cumula- tive proximal outcomes over existing algorithms. Theoretical results on regret bounds are also derived. The countts R package provides an implementa- tion of our approach.","tags":["Mobile health","Just-in-time adaptive interventions","Thompson sampling","Multi-armed bandits"],"title":"Thompson Sampling for Zero-Inflated Count Outcomes With an Application to the Drink Less Mobile Health Study","type":"publication"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1734393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734393600,"objectID":"7b48c18e7343a02b6a50251ff19585ef","permalink":"https://nina-dl.github.io/talk/2024_icsds/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/talk/2024_icsds/","section":"talk","summary":"It is now commonly known that using optimal response-adaptive designs for data collection offers great potential in terms of optimizing expected outcomes, but poses multiple challenges for inferential goals. In many settings, such as phase-II or confirmatory clinical trials, a main barrier to their practical use is the lack of type-I error guarantees and/or power efficiency, especially in finite samples. This work addresses this gap. Specifically, focusing on a novel test statistic defined on the randomization probabilities of the (randomized) adaptive design, we derive its finite-sample and asymptotic guarantees. Further theoretical properties are evaluated under a Bayesian response-adaptive design, which is commonly used both in clinical trial applications and beyond (e.g., recommendation systems or mobile health). The frequentist error control advantages of the proposed approach–also able to preserve expected outcome optimalities–are illustrated in a real-world phase-II oncology trial and in simulation experiments.","tags":["Response-adaptive designs","Thompson sampling","Statistical inference"],"title":"Finite-sample and Asymptotic Error Control of a Novel Test for Response-adaptive Designs (Invited Talk @ ICSDS2024)","type":"talk"},{"authors":["Nina Deliu","Joseph Jay Williams","Bibhas Chakraborty"],"categories":null,"content":"","date":1734048000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734048000,"objectID":"ab0c619b52e68176fdd267ea45907a55","permalink":"https://nina-dl.github.io/publication/2024rlreview_isr/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/2024rlreview_isr/","section":"publication","summary":"In recent years, reinforcement learning (RL) has acquired a prominent position in health-related sequential decision-making problems, gaining traction as a valuable tool for delivering adaptive interventions (AIs). However, in part due to a poor synergy between the methodological and the applied communities, its real-life application is still limited and its potential is still to be realized. To address this gap, our work provides the first unified technical survey on RL methods, complemented with case studies, for constructing various types of AIs in healthcare. In particular, using the common methodological umbrella of RL, we bridge two seemingly different AI domains, dynamic treatment regimes and just-in-time adaptive interventions in mobile health, highlighting similarities and differences between them and discussing the implications of using RL. Open problems and considerations for future research directions are outlined. Finally, we leverage our experience in designing case studies in both areas to showcase the significant collaborative opportunities between statistical, RL, and healthcare researchers in advancing AIs.","tags":["Dynamic treatment regimes","Just-in-time adaptive interventions","Reinforcement learning","Biostatistics"],"title":"Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions","type":"publication"},{"authors":["Nina Deliu","Bibhas Chakraborty"],"categories":null,"content":"","date":1733961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733961600,"objectID":"5c1056c1366524103d3a2bdf2dd0d4ab","permalink":"https://nina-dl.github.io/publication/2024ai_bookch/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/2024ai_bookch/","section":"publication","summary":"Precision health, increasingly supported by digital technologies, is a domain of research that broadens the paradigm of precision medicine, advancing everyday healthcare. This vision goes hand in hand with the groundbreaking advent of artificial intelligence (AI), which is reshaping the way we diagnose, treat, and monitor both clinical subjects and the general population. AI tools powered by machine learning have shown considerable improvements in a variety of healthcare domains. In particular, reinforcement learning (RL) holds great promise for sequential and dynamic problems such as dynamic treatment regimes and just-in-time adaptive interventions in digital health. In this work, we discuss the opportunity offered by AI, more specifically RL, to current trends in healthcare, providing a methodological survey of RL methods in the context of precision and digital health. Focusing on the area of adaptive interventions, we expand the methodological survey with illustrative case studies that used RL in real practice.","tags":["Precision health","Reinforcement learning","Artificial intelligence","Dynamic treatment regimes"],"title":"Artificial Intelligence-based Decision Support Systems for Precision and Digital Health","type":"publication"},{"authors":["Nina Deliu","Brunero Liseo"],"categories":null,"content":"","date":1732838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732838400,"objectID":"4f2d22d6ab83cdfa14ba460c0240fbcf","permalink":"https://nina-dl.github.io/publication/2024compstat/","publishdate":"2024-11-29T00:00:00Z","relpermalink":"/publication/2024compstat/","section":"publication","summary":"Doping control is an essential component of anti-doping organizations for protecting sport competitions. Since 2009, this mission has been complemented worldwide by the Athlete Biological Passport (ABP), used to monitor athletes’ individual profiles over time. The practical implementation of the ABP is based on a Bayesian framework, called ADAPTIVE, intended to identify individual reference ranges outside of which an observation may indicate doping abuse. Currently, this method follows a univariate approach, relying on simultaneous analysis of different markers. This work extends the ADAPTIVE method to a multivariate testing framework, making use of copula models to couple the marginal distribution of biomarkers with their dependence structure. After introducing the proposed copula-based hierarchical model, we discuss our approach to inference, grounded in a Bayesian spirit, and present an extension to multidimensional predictive reference regions. Focusing on the hematological module of the ABP, we evaluate the proposed framework in both data-driven simulations and real data.","tags":["Bayesian inference","Copula models","Doping detection"],"title":"A copula-based Bayesian framework for doping detection","type":"publication"},{"authors":["Nina Deliu","Brunero Liseo"],"categories":null,"content":"","date":1725926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725926400,"objectID":"0769882a5e0a411f8ed006eb2b3f94d9","permalink":"https://nina-dl.github.io/talk/2024_copa/","publishdate":"2024-06-02T00:00:00Z","relpermalink":"/talk/2024_copa/","section":"talk","summary":"This work addresses the problem of detecting anomalies or abnormal values in multivariate longitudinal data. Specifically, motivated by the international mission of antidoping agencies, we are interested in identifying potential doping abuse in sports by analyzing athletes’ individual profiles over time (WADA, 2021). In practice, the goal is to sequentially construct, at each time measurement t and for each athlete, individual reference regions, against which a new observed sample can be compared. We propose a solution based on conformal predictive inference within a multivariate Bayesian hierarchical framework. To facilitate tractable and accurate estimation of multivariate profiles, we leverage the use of copula models, which allow for separate modeling of the marginal distributions and their dependency structure (Nelsen, 2006). Bayesian principles are employed to estimate the multivariate predictive distribution, and the correspondent multidimensional reference region is derived using a conformal theory approach (Vovk et al., 2005). We show that the resulting region is equivalent to the highest-predictive region when the predictive distribution is used as a conformal measure. Extensive simulation studies are performed to showcase the advantages of the proposed copula-based solution over other existing univariate and multivariate frameworks, including scenarios of misspecification. Furthermore, this approach offers a flexible alternative with the possibility of integrating any parametric distribution for the marginals and potentially complex dependency structure through different existing copula models. The practical implementation and relevance of this framework are finally demonstrated in real profile data on a control (non-doped) population and on a doped athlete.","tags":["Anomaly detection","Bayesian hierarchical modeling","Copula models","Highest-density regions","Posterior predictive density","Reference ranges","Conformal inference"],"title":"Anomaly Detection in Multivariate Profiles with Conformal Bayesian Inference (Poster @ COPA2024)","type":"talk"},{"authors":["Nina Deliu","Brunero Liseo"],"categories":null,"content":"","date":1723420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723420800,"objectID":"ff5714cc6f1b87806552b26b2d355ac9","permalink":"https://nina-dl.github.io/publication/2024irs_hdr/","publishdate":"2024-08-12T00:00:00Z","relpermalink":"/publication/2024irs_hdr/","section":"publication","summary":"Among the variety of statistical intervals, highest-density regions (HDRs) stand out for their ability to effectively summarise a distribution or sample, unveiling its distinctive and salient features. An HDR represents the minimum size set that satisfies a certain probability coverage, and current methods for their computation require knowledge or estimation of the underlying probability distribution or density $f$. In this work, we illustrate a broader framework for computing HDRs, which generalises the classical density quantile method. The framework is based on neighbourhood measures, that is, measures that preserve the order induced in the sample by $f$, and include the density as a special case. We explore a number of suitable distance-based measures, such as the $k$-nearest neighbourhood distance, and some probabilistic variants based on copula models. An extensive comparison is provided, showing the advantages of the copula-based strategy, especially in those scenarios that exhibit complex structures (e.g. multimodalities or particular dependencies). Finally, we discuss the practical implications of our findings for estimating HDRs in real-world applications.","tags":["Highest-density regions","Uncertainty quantification","Copula models"],"title":"Alternative Approaches for Estimating Highest-Density Regions","type":"publication"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1718841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718841600,"objectID":"b87f638b6764b5b568af079989dd0c0b","permalink":"https://nina-dl.github.io/talk/2024_bmsaned/","publishdate":"2024-06-03T00:00:00Z","relpermalink":"/talk/2024_bmsaned/","section":"talk","summary":"The multi-armed bandit (MAB) framework holds great promise for optimizing sequential decisions online as new data arise. For example, it could be used to design adaptive experiments that can result in better participant outcomes and improved statistical power at the end of the study. However, due to mathematical and computational aspects, most MAB variants have been developed and are implemented under binary or normal outcome models. In this talk, guided by three biomedical case studies we have designed, I will illustrate how traditional statistics can be integrated within this framework to enhance its potential. Specifically, I will focus on the most popular Bayesian MAB algorithm, Thompson sampling, and on two types of outcomes: (i) rating scales, increasingly common in recommendation systems, digital health and education, and (ii) zero-inflated data, characterizing mobile health experiments. Theoretical properties and empirical advantages in terms of balancing exploitation (outcome performance) and exploration (learning performance) will be presented. Further considerations will be provided in the unique and challenging case of (iii) small samples. These works are the result of collaborations with Sofia Villar (Cambridge University), Bibhas Chakraborty (NUS University) and the IAI Lab (Toronto University), among others.","tags":["Multi-armed bandits","Online reinforcement learning","Adaptive designs","Thompson sampling","Zero-inflated data","Rating scales","Small samples"],"title":"Online sequential-decision making via bandit algorithms, modeling considerations for better decisions (Invited Talk @ BMS-ANed)","type":"talk"},{"authors":["Nina Deliu","Joseph J Williams","Sofia Villar"],"categories":null,"content":"","date":1718668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718668800,"objectID":"014daf7b766d69a47a4c0d7b9fb41fb1","permalink":"https://nina-dl.github.io/talk/2024_sis/","publishdate":"2024-06-04T00:00:00Z","relpermalink":"/talk/2024_sis/","section":"talk","summary":"Using bandit algorithms to design response-adaptive trials can optimize participant outcomes, but poses major challenges for statistical inference. Recent attempts to address these challenges typically impose restrictions on the exploitative nature of the bandit algorithm and require large sample sizes to ensure asymptotic guarantees. However, large experiments generally follow a successful pilot study, which is tightly constrained in its size or duration. In this work, we tackle the problem of hypothesis testing in finite samples. We illustrate an innovative hypothesis testing procedure, uniquely based on the allocation probabilities of the bandit algorithm, and theoretically characterise it when applied to Thompson sampling.","tags":["adaptive designs","bandit algoritms","exact tests"],"title":"Finite-sample inference in response-adaptive designs. An application to Thompson sampling (Invited Talk @ SIS2024)","type":"talk"},{"authors":["Nina Deliu","Brunero Liseo"],"categories":null,"content":"","date":1716422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716422400,"objectID":"ec18994b43a23d3b34fb8bcc332743e7","permalink":"https://nina-dl.github.io/talk/2024_saw/","publishdate":"2024-05-23T00:00:00Z","relpermalink":"/talk/2024_saw/","section":"talk","summary":"Doping control is an essential component of anti-doping organizations for protecting clean sport competitions. Since 2009, this mission has been complemented worldwide by the Athlete Biological Passport (ABP), used to monitor athletes' individual profiles over time. The practical implementation of the ABP is based on a Bayesian framework, called ADAPTIVE (Sottas et al., 2008), intended to identify individual reference ranges outside of which an observation may indicate doping abuse. Currently, this method follows a univariate approach, relying on simultaneous analysis of different markers. This work extends the ADAPTIVE method to a multivariate testing framework, making use of copula models to couple the marginal distribution of biomarkers with their dependence structure. After introducing the proposed copula-based hierarchical model, we discuss our approach to inference, grounded in a Bayesian spirit, and present a conformal method for constructing predictive reference regions (Vovk et al., 2005). More in detail, we use, as a conformal measure, the posterior predictive density of the multidimensional biomarkers of individual athletes. Focusing on the haematological module of the ABP, we evaluate the proposed framework in both data-driven simulations and real data.  This is a joint work with Brunero Liseo. ","tags":["Bayesian hierarchical modeling","Copula models","Highest-density regions","Posterior predictive density","Reference ranges","Conformal inference"],"title":"A Multivariate Copula-based Conformal Bayesian Framework for Doping Detection  (Talk @ AUEB-SAW2024)","type":"talk"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1715212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715212800,"objectID":"faaf8211a0cff81b54399c0d8b13ee3a","permalink":"https://nina-dl.github.io/talk/2024_padovaseminar/","publishdate":"2024-05-09T00:00:00Z","relpermalink":"/talk/2024_padovaseminar/","section":"talk","summary":"The multi-armed bandit (MAB) framework holds great promise for optimizing sequential decisions online as new data arise. For example, it could be used to design adaptive experiments that can result in better participant outcomes and improved statistical power at the end of the study. However, due to mathematical and computational aspects, most MAB variants have been developed and are implemented under binary or normal outcome models. In this talk, guided by three biomedical case studies we have designed, I will illustrate how traditional statistics can be integrated within this framework to enhance its potential. Specifically, I will focus on the most popular Bayesian MAB algorithm, Thompson sampling, and on two types of outcomes: (i) rating scales, increasingly common in recommendation systems, digital health and education, and (ii) zero-inflated data, characterizing mobile health experiments. Theoretical properties and empirical advantages in terms of balancing exploitation (outcome performance) and exploration (learning performance) will be presented. Further considerations will be provided in the unique and challenging case of (iii) small samples. These works are the result of collaborations with Sofia Villar (Cambridge University), Bibhas Chakraborty (NUS University) and the IAI Lab (Toronto University), among others.","tags":["Multi-armed bandits","Online reinforcement learning","Adaptive designs","Thompson sampling","Zero-inflated data","Rating scales","Small samples"],"title":"Online sequential-decision making via bandit algorithms, modeling considerations for better decisions (Seminar @ Department of Statistics, Padua University)","type":"talk"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1712707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712707200,"objectID":"b9bc1a43c818a59d31f0f6b33729e4bf","permalink":"https://nina-dl.github.io/talk/2024_albecs/","publishdate":"2024-04-10T00:00:00Z","relpermalink":"/talk/2024_albecs/","section":"talk","summary":"The multi-armed bandit (MAB) framework holds great promise for optimizing sequential decisions online as new data arise. For example, it could be used to design adaptive experiments that can result in better participant outcomes and improved statistical power at the end of the study. However, due to mathematical and computational aspects, most MAB variants have been developed and are implemented under binary or normal outcome models. In this talk, guided by three biomedical case studies we have designed, I will illustrate how traditional statistics can be integrated within this framework to enhance its potential. Specifically, I will focus on the most popular Bayesian MAB algorithm, Thompson sampling, and on two types of outcomes: (i) rating scales, increasingly common in recommendation systems, digital health and education, and (ii) zero-inflated data, characterizing mobile health experiments. Theoretical properties and empirical advantages in terms of balancing exploitation (outcome performance) and exploration (learning performance) will be presented. Further considerations will be provided in the unique and challenging case of (iii) small samples. These works are the result of collaborations with Sofia Villar (Cambridge University), Bibhas Chakraborty (NUS University) and the IAI Lab (Toronto University), among others.","tags":["Multi-armed bandits","Online reinforcement learning","Adaptive designs","Thompson sampling","Zero-inflated data","Rating scales","Small samples"],"title":"Online sequential-decision making via bandit algorithms, modeling considerations for better decisions (Keynote Talk @ ALBECS-2024, 19th International Conference on Persuasive Technology 2024)","type":"talk"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1711497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711497600,"objectID":"42dde93927938b16a4b31d2e238875f0","permalink":"https://nina-dl.github.io/talk/2024_gdr/","publishdate":"2024-03-27T00:00:00Z","relpermalink":"/talk/2024_gdr/","section":"talk","summary":"","tags":["Thompson sampling","Zero-inflated count outcomes","Reinforcement learning"],"title":"Reinforcement learning for sequential decision-making. From healthcare to finance (Talk @ Sapienza University)","type":"talk"},{"authors":["Xueqing Liu","Nina Deliu","Tanujit Chakraborty","Lauren Bell","Bibhas Chakraborty"],"categories":null,"content":"","date":1709164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709164800,"objectID":"c2bca3b79744339ba8929ac72be36802","permalink":"https://nina-dl.github.io/talk/2024_rar/","publishdate":"2024-02-29T00:00:00Z","relpermalink":"/talk/2024_rar/","section":"talk","summary":"Mobile health (mHealth) technologies aim to improve distal outcomes, such as clinical conditions, by optimizing proximal outcomes through just-in-time adaptive interventions. Contextual bandits provide a suitable framework for customizing such interventions, however, unique challenges such as modeling count outcomes persist within existing bandit frameworks. We address this challenge by leveraging count data models into online bandit approaches for micro-randomized trials. Specifically, we focus on a popular contextual bandit algorithm called Thompson sampling. We present theoretical results on our proposed strategies, as well as empirical results both in simulations and on a real dataset from the Drink Less trial. While the focus of this work is on design aspects, we conclude by discussing the inference problem in adaptively-collected data through a novel statistical test based on the allocation probabilities.","tags":["Thompson sampling","Zero-inflated count outcomes","Mobile health"],"title":"Thompson sampling for count data in mobile health trials (Invited Talk @ University of Cambridge)","type":"talk"},{"authors":["Xinru Wang","Nina Deliu","Yusuke Narita","Bibhas Chakraborty"],"categories":null,"content":"","date":1706832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706832000,"objectID":"143bf0c7522a5ccc8ddfdd508659ad11","permalink":"https://nina-dl.github.io/publication/2024smartexam_biometrics/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/2024smartexam_biometrics/","section":"publication","summary":"Dynamic treatment regimes (DTRs) are sequences of decision rules that recommend treatments based on patients' time-varying clinical conditions. The sequential multiple assignment randomized trial (SMART) is an experimental design that can provide high-quality evidence for constructing optimal DTRs. In a SMART, participants are randomized to available treatments at multiple stages, typically following a fixed and balanced randomization procedure. Despite its relative simplicity of implementation and desirable performance in comparing embedded DTRs, the SMART with balanced randomization (SMART-BR) is faced with inevitable ethical issues including assigning many participants to the observed inferior treatment or the treatment they dislike, which might slow down the recruitment procedure and lead to higher attrition rates. In this context, we propose a SMART under the Experiment-as-Market framework (SMART-EXAM), a novel SMART design that holds the potential to improve patient welfare by incorporating participants' preferences and predicted treatment effects into the randomization procedure. We describe the procedure of conducting a SMART-EXAM and evaluate its theoretical and empirical statistical properties compared with other competing SMART designs. The results indicate that the SMART-EXAM design can improve the welfare of participants enrolled in the trial, while also achieving a comparable ability to construct an optimal DTR. We finally illustrate the practical potential of the SMART-EXAM design using data from a SMART for children with attention-deficit/hyperactivity disorder (ADHD).","tags":["SMART design","Dynamic treatment regimes","Precision medicine","Trial design","Participant welfare"],"title":"Incorporating Participants' Welfare into Sequential Multiple Assignment Randomized Trials","type":"publication"},{"authors":["Harsh Kumar","Tong Li","Jiakai Shi","Ilya Musabirov","Rachel Kornfield","Jonah Meyerhoff","Ananya Bhattacharjee","Chris Karr","Theresa Nguyen","David Mohr","Anna Rafferty","Sofia Villar","Nina Deliu","Joseph Jay Williams"],"categories":null,"content":"","date":1706745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"bcf825540767135aa5d6a653648edcbe","permalink":"https://nina-dl.github.io/publication/2024mha/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/2024mha/","section":"publication","summary":"Digital mental health (DMH) interventions, such as text-message-based lessons and activities, offer immense potential for accessible mental health support. While these interventions can be effective, real-world experimental testing can further enhance their design and impact. Adaptive experimentation, utilizing algorithms like Thompson Sampling for (contextual) multi-armed bandit (MAB) problems, can lead to continuous improvement and personalization. However, it remains unclear when these algorithms can simultaneously increase user experience rewards and facilitate appropriate data collection for social-behavioral scientists to analyze with sufficient statistical confidence. Although a growing body of research addresses the practical and statistical aspects of MAB and other adaptive algorithms, further exploration is needed to assess their impact across diverse real-world contexts. This paper presents a software system developed over two years that allows text-messaging intervention components to be adapted using bandit and other algorithms while collecting data for side-by-side comparison with traditional uniform random non-adaptive experiments. We evaluate the system by deploying a text-message-based DMH intervention to 1100 users, recruited through a large mental health non-profit organization, and share the path forward for deploying this system at scale. This system not only enables applications in mental health but could also serve as a model testbed for adaptive experimentation algorithms in other domains.","tags":["Adaptive experiments","Thompson sampling","Multi-armed bandits","Mental health"],"title":"Using Adaptive Bandit Experiments to Increase and Investigate Engagement in Mental Health","type":"publication"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1704499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704499200,"objectID":"6ecd3e08c6fe1319b5815b4dadc83a23","permalink":"https://nina-dl.github.io/publication/2023smap/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/2023smap/","section":"publication","summary":"Bandit algorithms such as Thompson sampling (TS) have been put forth for decades as useful tools for conducting adaptively-randomised experiments. By skewing the allocation toward superior arms, they can substantially improve particular outcomes of interest for both participants and investigators. For example, they may use participants’ ratings for continuously optimising their experience with a program. However, most of the bandit and TS variants are based on either binary or continuous outcome models, leading to suboptimal performances in rating scale data. Guided by behavioural experiments we conducted online, we address this problem by introducing Multinomial-TS for rating scales. After assessing its improved empirical performance in unique optimal arm scenarios, we explore potential considerations (including prior's role) for calibrating uncertainty and balancing arm allocation in scenarios with no unique optimal arms.","tags":["Adaptive experiments","Thompson sampling","Multi-armed bandits","Rating scales","Multinomial model","Dirichlet distribution","Incomplete learning"],"title":"Multinomial Thompson sampling for rating scales and prior considerations for calibrating uncertainty","type":"publication"},{"authors":["Nina Deliu","Bibhas Chakraborty"],"categories":null,"content":"","date":1703116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703116800,"objectID":"d61d3f293b37b2ccc5424c6804cbbdfe","permalink":"https://nina-dl.github.io/talk/2023_icsds/","publishdate":"2023-12-21T00:00:00Z","relpermalink":"/talk/2023_icsds/","section":"talk","summary":"Artificial intelligence tools powered by machine learning have shown considerable improvements in a variety of experimental domains, from education to healthcare. In particular, the reinforcement learning (RL) and the multi-armed bandit (MAB) framework hold great promise for defining sequential designs with the aim of delivering optimized adaptive interventions with outcomes and resource (e.g., cost, time, or sample size) benefits. In this work, we discuss the opportunity offered by RL and MABs to current trends in healthcare experimentation, as well as the specific modeling challenges posed by this framework. Motivated by three case studies--in mobile health, digital mental health, and clinical trials--differing in their type of outcome, we illustrate our methodological contribution to this framework by integrating elements of traditional statistics. Specifically, we combine common offline data models for count and rating scale outcomes, increasingly common in digital and mobile health, with the Thompson sampling technique, which is possibly the most popular MAB algorithm. We discuss the theoretical properties of some of the proposed solutions and evaluate their empirical advantages in terms of balancing the exploitation (outcome performance) and exploration (learning performance) trade-off typical of reinforcement learning problems. Further considerations are provided under the unique challenging case of small samples, where parametric assumptions are often unrealistic. In such settings, we demonstrate how RL-based solutions combined with bootstrap approaches represent a flexible yet improved strategy for achieving a near-optimal balance between patient benefit within the study and enhancing statistical operating characteristics in a small population.","tags":["adaptive designs","Thomspon sampling","multi-armed bandits","inference"],"title":"Modeling considerations when optimizing adaptive experiments under the reinforcement learning framework (Invited Talk @ ICSDS2023)","type":"talk"},{"authors":["Ananya Bhattacharjee","Haochen Song","Xuening Wu","Justice Tomlinson","Mohi Reza","Akmar Ehsan Chowdhury","Nina Deliu","Thomas Price","Joseph Jay Williams"],"categories":null,"content":"","date":1698969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698969600,"objectID":"8e32b924cd070e061ba34faf4619d258","permalink":"https://nina-dl.github.io/publication/2023missing_hcomp/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/2023missing_hcomp/","section":"publication","summary":"Machine learning algorithms often require quantitative ratings from users to effectively predict helpful content. When these ratings are unavailable, systems make implicit assumptions or imputations to fill in the missing information; however, users are generally kept unaware of these processes. In our work, we explore ways of informing the users about system imputations and experiment with imputed ratings and various explanations required by users to correct imputations. We investigate these approaches through the deployment of a text messaging probe to 26 participants to help them manage their psychological wellbeing. We provide quantitative results to report users' reactions to correct vs. incorrect imputations and the potential risks of biasing their ratings. Using semi-structured interviews with participants,  we characterize the potential trade-offs regarding user autonomy and draw insights about alternative ways of involving users in the imputation process. Our findings provide useful directions for future research on communicating imputation and interpreting user non-responses.","tags":["Data imputation","Human computer interaction"],"title":"Informing Users about Data Imputation. Exploring the Design Space for Dealing With Non-Responses","type":"publication"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1698883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698883200,"objectID":"9ba00690fb2af53d10665aa4548c3449","permalink":"https://nina-dl.github.io/publication/2023rldemography_ququ/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/2023rldemography_ququ/","section":"publication","summary":"Reinforcement learning (RL) algorithms have been long recognized as powerful tools for optimal sequential decision making. The framework is concerned with a decision maker, the agent, that learns how to behave in an unknown environment by making decisions and seeing their associated outcome. The goal of the RL agent is to infer, through repeated experience, an optimal decision-making policy, i.e., a sequence of action rules that would lead to the highest, typically long-term, expected utility. Today, a wide range of domains, from economics to education and healthcare, have embraced the use of RL to address specific problems. To illustrate, we used an RL-based algorithm to design a text-messaging system that delivers personalized real-time behavioural recommendations to promote physical activity and manage depression. Motivated by the recent call of the UNECE for government-wide actions to adapt to population ageing, in this work, we argue that the RL framework may provide a set of compelling strategies for supporting population research and informing population policies. After introducing the RL framework, we discuss its potential in three population studies applications: international migration, public health, and fertility.","tags":["Reinforcement learning","Artificial intelligence","Population studies","Migration policies","Population policies"],"title":"Reinforcement learning for sequential decision making in population research","type":"publication"},{"authors":["Nina Deliu","Sofia Villar"],"categories":null,"content":"","date":1694736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694736000,"objectID":"744a6a93d8239774fb1923b104b2a41a","permalink":"https://nina-dl.github.io/talk/2023_statalk/","publishdate":"2023-09-15T00:00:00Z","relpermalink":"/talk/2023_statalk/","section":"talk","summary":"Response-adaptive designs, either based on simple rules, urn models, or bandit problems, are of increasing interest among both theoretical and practical communities. In particular, regret-optimising bandit algorithms like Thompson sampling hold the great promise to optimise an experimental design by limiting the exposure to inferior arms and enhancing participants' outcomes. However, their practical potential in biomedical settings is yet to be realised. In fact, the analysis of adaptively-collected data has exposed the inadequacy of traditional statistics for robust and reliable conclusions, with considerable issues in hypothesis testing. Attempts to develop valid testing procedures are predominantly focused on type-I error control, are very inefficient in terms of power–especially in small samples–and their applicability is limited to specific design characteristics. This work addresses this gap by illustrating an alternative approach to statistical testing that can be in principle applied to any randomised algorithm for which the randomisation probabilities are available. Finite-sample and asymptotic properties of this solution are discussed both in a general setting and under the specific Thompson sampling design. The empirical advantages of the proposed approach–able to preserve regret optimalities, while offering inferential benefits–are finally illustrated in empirical examples.","tags":["adaptive designs","Thomspon sampling","multi-armed bandits","inference","allocation-probability test"],"title":"On the finite-sample and asymptotic validity of an allocation-probability test for adaptively-collected data (Invited Talk @ StaTalk2023)","type":"talk"},{"authors":["Nina Deliu","Rowena Jones","Mark Toshner","Sofia S Villar"],"categories":null,"content":"","date":1694131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694131200,"objectID":"37838b522ad01d1f0cb5da2b7ea00dc9","permalink":"https://nina-dl.github.io/talk/2023_bayesianday/","publishdate":"2023-09-08T00:00:00Z","relpermalink":"/talk/2023_bayesianday/","section":"talk","summary":"Designing early- and late-phase clinical trials for rare conditions pose unique challenges, ranging from ethical considerations to generating robust inferences from limited sample sizes. In this work, guided by a rare-disease precision-medicine case study, we outline the potential of innovative designs for addressing the above challenges in small populations. We build on a response-adaptive randomization framework and incorporate Bayesian principles for utilising the continuously accrued responses to skew the allocation probabilities toward the most promising arms. Using prior patient data, extensive simulations are carried out for sample size evaluation with 20 patients per stratum. Compared to a traditional balanced design, the flexibility of the proposed framework results in substantial gains in both statistical power and a higher chance for patients to receive the superior arm.","tags":["response-adaptive design","Bayesian design","Pulmonary arterial hypertension","rare disease"],"title":"Enhancing patient outcomes and statistical efficiency in rare-disease phase-II trials. The StratosPHere 2 study (Invited Talk @ Italian Bayesian Day 2023)","type":"talk"},{"authors":["Nina Deliu","Rowena Jones","Mark Toshner","Sofia S Villar"],"categories":null,"content":"","date":1693267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693267200,"objectID":"07d51e2af0b21752a2fc19533d92f53e","permalink":"https://nina-dl.github.io/talk/2023_iscb/","publishdate":"2023-08-29T00:00:00Z","relpermalink":"/talk/2023_iscb/","section":"talk","summary":"Designing early- and late-phase clinical trials for rare conditions pose unique challenges, ranging from ethical and practical enrolment considerations to generating robust evidence from limited sample sizes. Pulmonary arterial hypertension (PAH), for example, is a life-limiting progressive disorder characterised by high blood pressure in the arteries of the lungs that affects 15 to 50 cases per million in the US and Europe. Although treatable, PAH has no available cure. In such a setting, conducting trials to primarily learn about treatment effectiveness (as in traditional fixed randomised designs) may result in an unacceptably low expected outcome for patients in presence of superior treatments. Further, meeting conventional power requirements, while controlling for type-I error, is infeasible in inherently small and heterogeneous populations. StratosPHere 2 is the first-ever precision-medicine trial of treatments targeting common genetic causes of the disease: BMPR2 mutations. Using this case study, this work outlines the potential of innovative designs for addressing the above challenges in small populations. The proposed design builds on a response-adaptive randomised framework and incorporates Bayesian principles for utilising the continuously accrued responses (transcriptomic biomarker expressions) to adapt the allocation probabilities. It involves three stages, each of which enrolling and allocating from 6 to 8 patients to one of the three study arms (two active and one control arm). The adaptive design is stratified by two BMPR2 mutation subgroups and performed independently in each stratum. In addition to varying the allocation probabilities from one stage to another, the design is allowed to drop one of the two active arms in case of early evidence of inferiority. Extensive simulations, using prior PAH patient data, are carried out to perform sample size evaluation with 20 patients per mutation stratum. Compared to a traditional fixed design, the flexibility of the proposed framework results in substantial gains in both statistical power and a higher chance for patients to receive the superior arm.","tags":["response-adaptive design","Bayesian design","Pulmonary arterial hypertension","rare disease"],"title":"Enhancing patient outcomes and statistical efficiency in rare-disease phase-II trials. The StratosPHere 2 study (Poster @ ISCB2023 - Joint conference with the IBS)","type":"talk"},{"authors":["Nina Deliu","Joseph Jay Williams","Sofia Villar"],"categories":null,"content":"","date":1691280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691280000,"objectID":"c6397082e6e63347c3111ebfce291010","permalink":"https://nina-dl.github.io/talk/2023_jsm/","publishdate":"2023-08-06T00:00:00Z","relpermalink":"/talk/2023_jsm/","section":"talk","summary":"Using bandit algorithms to conduct adaptive randomised experiments can minimise regret, but it poses major challenges for statistical inference. Recent attempts to address these challenges typically impose restrictions on the exploitative nature of the bandit algorithm–trading off regret–and require large sample sizes to ensure asymptotic guarantees. However, large experiments generally follow a successful pilot study, which is tightly constrained in its size or duration. Increasing power in such small pilot experiments, without limiting the adaptive nature of the algorithm, can allow promising interventions to reach a larger experimental phase. In this work we introduce a novel hypothesis test, uniquely based on the allocation probabilities of the bandit algorithm, and without constraining its exploitative nature or requiring a minimum experimental size. We characterise our Allocation Probability Test when applied to Thompson Sampling, presenting its theoretical properties, and illustrating its finite-sample performances compared to state-of-the-art approaches. We show the regret and inferential advantages of our approach in both simulations and in a real-world experiment.","tags":["adaptive designs","Thomspon sampling","multi-armed bandits","inference","allocation-probability test"],"title":"Efficient Inference Without Trading-off Regret in Bandits. An Allocation Probability Test for Thompson Sampling (Invited Talk @ JSM2023)","type":"talk"},{"authors":["Nina Deliu","Brunero Liseo"],"categories":null,"content":"","date":1691020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691020800,"objectID":"7815ca467e31d0b511800e6a7f3a3821","permalink":"https://nina-dl.github.io/talk/2023_ecosta/","publishdate":"2023-08-03T00:00:00Z","relpermalink":"/talk/2023_ecosta/","section":"talk","summary":"Many statistical problems require estimating a density function, say f , from data samples. Multivariate highest-density regions (HDRs) are considered - i.e., minimum volume sets containing a given probability - typically computed using a density quantile approach. Nevertheless, the density estimation task is far from trivial, especially over increased dimensions and when data are sparse and exhibit complex structures (e.g., multimodalities or particular dependencies). This challenge is addressed by exploring alternative approaches to build HDRs that overcome direct multivariate density estimation. First, the density quantile method - currently implementable based on a consistent density estimator - is generalized to neighbourhood measures, i.e., measures that preserve the order induced in the sample by f. Second, it is elaborated on, and several suitable probabilistic - and distance-based measures are evaluated, such as the k-neighbourhood Euclidean distance. Third, motivated by the ubiquitous role of copula modelling in modern statistics, its use in probabilistic-based measures is explored. By separately modelling marginals and their (potentially complex) dependence structure - that is the copula - the multivariate density estimation and better capture data specificities can both be relaxed. Finally, a comprehensive comparison among the introduced measures is provided, and their implications for computing HDRs in real-world problems are discussed.","tags":["Highest density regions","Copula modelling","Kernel density estimation"],"title":"Probabilistic and Distance-based Approaches for Computing Highest-Density Regions (Talk @ EcoSta2023)","type":"talk"},{"authors":["Nina Deliu","Brunero Liseo"],"categories":null,"content":"","date":1687824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687824000,"objectID":"9f4abc7496b27088da3ccb41f840c216","permalink":"https://nina-dl.github.io/talk/2023_gdr/","publishdate":"2023-06-27T00:00:00Z","relpermalink":"/talk/2023_gdr/","section":"talk","summary":"We investigate the problem of deriving highest density regions (HDRs) from multivariate data samples. We are interested in estimating minimum volume sets that contain a given probability. In the case of unknown distribution probabilities f, the problem involves their estimation, which may be challenging in multidimensional settings. Motivated by the ubiquitous role of copula modelling in modern statistics, we explore their use in the context of HDR estimation. Rather than directly estimating the multivariate f, we propose to estimate the marginals and their dependence structure, i.e., the copula structure, separately. We evaluate this new method, using both a parametric and a nonparametric approach, in a number of synthetic experiments and considering a real dataset.","tags":["Highest density regions","Copula modelling","Kernel density estimation"],"title":"Computing Highest Density Regions with Copulae (Poster @ 13 GdR MEMOTEF)","type":"talk"},{"authors":["Nina Deliu","Brunero Liseo"],"categories":null,"content":"","date":1687478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687478400,"objectID":"6bd70aa52b37d915d607dd3f3db4d7e5","permalink":"https://nina-dl.github.io/talk/2023_sis/","publishdate":"2023-06-23T00:00:00Z","relpermalink":"/talk/2023_sis/","section":"talk","summary":"We investigate the problem of deriving highest density regions (HDRs) from multivariate data samples. We are interested in estimating minimum volume sets that contain a given probability. In the case of unknown distribution probabilities f, the problem involves their estimation, which may be challenging in multidimensional settings. Motivated by the ubiquitous role of copula modelling in modern statistics, we explore their use in the context of HDR estimation. Rather than directly estimating the multivariate f, we propose to estimate the marginals and their dependence structure, i.e., the copula structure, separately. We evaluate this new method, using both a parametric and a nonparametric approach, in a number of synthetic experiments and considering a real dataset.","tags":["Highest density regions","Copula modelling","Kernel density estimation"],"title":"Computing Highest Density Regions with Copulae (Talk @ SIS 2023)","type":"talk"},{"authors":["Nina Deliu","Piero Demetrio Falorsi","Stefano Falorsi","Romina Filippini","Simona Toti","Diego Chianella","Giorgio Alleva"],"categories":null,"content":"","date":1686182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686182400,"objectID":"c58213b3beddc0f59e134e1c99c842eb","permalink":"https://nina-dl.github.io/talk/2023_itacosm/","publishdate":"2023-06-08T00:00:00Z","relpermalink":"/talk/2023_itacosm/","section":"talk","summary":"The Italian National Statistical Institute (ISTAT) is undergoing a significant modernization process, transitioning from a statistical paradigm based uniquely on independent sample and census surveys to an integrated system of statistical registries (ISSR). The ISSR is the result of a comprehensive integration of administrative and survey data and should represent the basis of all official statistics. Clearly, to allow the ISSR to be utilized as a single informative infrastructure, different statistical techniques (such as record linkage, statistical matching, or imputation/prediction) have been adopted to achieve the integration goal and reconstruct unit-level data. To illustrate, the reconstruction of the attained level of education was achieved by means of a sequence of log-linear models to impute the education level, combining the different informative contents of administrative data (provided by the Ministry of Education, University and Research; MIUR), census data (the 2011 Italian census), and sample survey data. It is thus the result of statistical processes subject to different sources of statistical uncertainty. These include, among others, the sampling errors due to sample surveys and the adoption of statistical models for imputing or predicting missing data at the unit level. The aim of this work is to evaluate, compare and propose novel methods for assessing the quality of data derived for specific cases of the ISSR. More specifically, our goal is to establish feasible computational statistical measures for calculating the accuracy of the provided estimates in the specific context of the attained level of education. In this work, we focus on the estimates of population totals, and allow for specific user-defined domains, e.g., the number of illiterate individuals in the province of Bologna. We build on the recent proposal described in Alleva et al. (2021: J Off Stat, 37, 481-503), where a new global measure for the estimation error, that is, the Global Mean Squared Error (GMSE), is developed, and assessed by simulations in the case of a logistic model. Accounting for two types of uncertainty, i.e., the sampling and the modeling uncertainty, we generalize the GMSE to multinomial response variables to resemble the categorical entity of the education level (overall 8 categories). The underlying strategy is based on three linearization steps, and it only involves the first two moments of the distribution. We evaluate the proposed method on a subsample of the Base Register of Individuals (BRI) of the ISSR and validate it with a bootstrap-based approach carried out on the same data. The GMSE results in a reliable, interpretable, computationally feasible, and flexible approach, which may allow different users to evaluate the accuracy of their own statistics produced from the ISSR. With this work, we aim to support the advancement of current practices in official statistics, facilitating a flexible yet correct use of registry data by augmenting the production of estimates with their corresponding quality.","tags":["accuracy estimation","uncertainty","register data"],"title":"Assessing the predictive accuracy of official-statistics registers. The Global Mean Squared Error measure (Invited Talk @ ITACOSM2023)","type":"talk"},{"authors":[],"categories":null,"content":"","date":1684800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684800000,"objectID":"4036352632ea3761c8f13c602da675f5","permalink":"https://nina-dl.github.io/talk/2023_napoliseminar/","publishdate":"2023-05-23T00:00:00Z","relpermalink":"/talk/2023_napoliseminar/","section":"talk","summary":"Multi-armed bandit algorithms such as Thompson sampling (TS) have been put forth for decades as useful tools for optimizing sequential decision-making in online experiments. By skewing the allocation ratio towards superior arms, they can minimize exposure to less promising arms and substantially improve participants' outcomes. For example, they may use continuously collected ratings to enhance their overall experience with a program. However, most TS variants assume either a binary or a normal outcome model, leading to suboptimal performances in rating scale data. Motivated by our real-world behavioral experiment, we introduce Multinomial-TS to more accurately incorporate rating scale preferences and redesign the study. Considerations on the prior’s role are further incorporated in order to mitigate the incomplete learning phenomenon typical of this Bayesian bandit algorithm.","tags":["adaptive experiments","multi-armed bandits","Thompson sampling","rating scales"],"title":"Multinomial Thompson Sampling for Online Sequential Decision Making with Rating Scales (Invited Seminar @ Federico II di Napoli)","type":"talk"},{"authors":[],"categories":null,"content":"# #Click on the Slides button above to view the built-in slides feature. #   #Slides can be added in a few ways:\n#- Create slides using Academic\u0026rsquo;s # Slides feature and link #using slides parameter in the front matter of the talk file #- Upload an existing slide deck to static/ and link using url_slides parameter in the front #matter of the talk file #- Embed your slides (e.g. Google Slides) or presentation video on this page using # shortcodes.\n#Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1683392700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683392700,"objectID":"80a2fbac8d380bea8823bd3cc0305abc","permalink":"https://nina-dl.github.io/talk/award_edu/","publishdate":"2023-05-06T17:05:00Z","relpermalink":"/talk/award_edu/","section":"talk","summary":"Our tool uses AI and ML to run adaptive experiments to determine and enable educators to employ the most effective teaching methods.","tags":[],"title":"We are officially the winners of the @xprize Digital Learning Challenge.","type":"talk"},{"authors":[],"categories":null,"content":"","date":1683244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683244800,"objectID":"89f0b5c2b04921c8301f84c8f1368d1e","permalink":"https://nina-dl.github.io/talk/2023_icnasta/","publishdate":"2023-05-05T00:00:00Z","relpermalink":"/talk/2023_icnasta/","section":"talk","summary":"Adaptive digital field experiments are continually increasing in their breadth of use in fields like mobile health and digital education. Using adaptive experimentation in education can help not only to explore and eventually compare various arms but also to direct more students to more helpful options. For example, they might explore whether one explanation type (e.g., critiquing an existing explanation or revising one’s own explanation) would lead students to gain a better understanding of a scientific concept and assign it more often. In such an experiment, data is rapidly and automatically analyzed to increase the proportion of future participants in the study allocated to better arms. One way of implementing adaptivity is through algorithms designed to solve multi-armed bandit (MAB) problems, such as Thompson Sampling (TS). The MAB problem is to effectively choose among K available options or arms, in order to maximize the expected outcome of interest (or reward). In this work, we present real-world case studies of applying TS in education. Specifically, we explore its use for motivating students, through different email reminders, to finalize their online homework. To evaluate the potential of MAB in education, we leverage the power of simulations to further explore the behavior of TS both when there is no difference between arms and when some difference exists. We empirically show that, while adaptive experiments can result in an increased benefit for students, by assigning more people to better arms, they can also cause problems for statistical analysis. Notably, this assignment strategy suggests an alert in drawing statistical conclusions, resulting in an inflated Type I error and a decreased power (failure to conclude there is a difference in arms when there truly is one). We explain why this happens and propose some strategies to mitigate these issues, in the hope to provide building blocks for future research to better balance the competing goals of reward maximization and statistical inference.","tags":["digital education","adaptive experiments","multi-armed bandits","Thompson sampling"],"title":"Adaptive Experiments for Enhancing Digital Education -- Benefits and Statistical Challenges (Talk @ ICNA-STA2023)","type":"talk"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"d18b863269e311284ffe79a79d12af2e","permalink":"https://nina-dl.github.io/publication/2023multiculturality_urbaniana/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/2023multiculturality_urbaniana/","section":"publication","summary":"This work examines the textual content of the focus group interviews conducted as part of the project “Multiculturality and education in Pontifical universities and formation communities of consecrated life”. More specifically, it focuses on the first focus group, with an in-depth analysis of the question “In your opinion, what is the difference between multiculturality and interculturality?”. The aim is to inves- tigate, by means of qualitative content analysis methods, participants’ under- standing and perspective of the two key concepts of this project, which are often misinterpreted or interchangeably misused. Results will show that participants have a well-clear idea of the concept of multiculturality, seen as a matter of fact of cultural plurality and diversity, and characterized by a definite and static nature. They also recognize that a multicultural plurality provides an opportunity for indi- vidual growth, but it must be regulated, especially at a communicative level, to allow for a mutually tolerant and respectful coexistence, without necessarily inter- fering with other cultures. On the contrary, in an intercultural context, it emerges the key role of union and mutual sharing, with a strong emphasis on individuals’ cultural transformation. In this regard, this contribution will bring light to a hetero- geneous and often conflicting perspective about the intensity of such transfor- mation. More specifically, to what extent individuals should preserve or lose their own cultural identities, as a result of the intercultural transformation process?","tags":[],"title":"Multiculturality and Interculturality: Qualitative Analysis of the Perspective of Focus Group Participants","type":"publication"},{"authors":null,"categories":null,"content":"It is increasingly common for data to be collected adaptively, where experimental costs are reduced progressively by assigning promising treatments more frequently. By skewing the allocation of the arms towards the more efficient or informative ones, they have the potential to enhance participants’ welfare, while resulting in a more flexible, efficient, and ethical alternative compared to traditional studies. However, such allocation strategies complicate the problem of statistical inference. It is now recognized that traditional inference methods are typically not valid when used in adaptively-collected data (such as those driven by multi-armed bandit problems), leading to considerable biases in classical estimators and other relevant issues in hypothesis testing problems.\n","date":1682553600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682553600,"objectID":"f9b4bbdf66ab53d243ccf9c136720127","permalink":"https://nina-dl.github.io/project/adaptive-data/","publishdate":"2023-04-27T00:00:00Z","relpermalink":"/project/adaptive-data/","section":"project","summary":"This project spans from statistical methodologies for performing inference in adaptively-collected data to designs for adaptive data collection","tags":["Inference"],"title":"Theory and methods for adaptively-collected data","type":"project"},{"authors":["Nina Deliu","Brunero Liseo"],"categories":null,"content":"","date":1677110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677110400,"objectID":"eec964c45bb4fb3e087bd016b6b4612b","permalink":"https://nina-dl.github.io/talk/2023_bsu/","publishdate":"2023-02-23T00:00:00Z","relpermalink":"/talk/2023_bsu/","section":"talk","summary":"Doping control, or testing, is one of the essential components adopted by anti-doping organizations to protect clean sport competitions. Alongside the evaluation of athletes’ samples for prohibited substances or methods, the Athlete Biological Passport (ABP) has been established, for some specific disciplines,  as a complementary pillar in the detection of doping, since its introduction in 2009. The fundamental principle of the ABP is to monitor over time athletes' individual profiles – with respect to certain doping biomarkers such as testosterone – that may reveal anti-doping rule violations. Significant variations from an athlete's established levels can be further assessed for possible manipulation. Currently, the practical implementation of the ABP framework is based on a Bayesian approach called ADAPTIVE. Specifically, given a biomarker, the predictive posterior distribution of a future sample is used to determine individual limits discriminating between normal and anomalous values. These individual limits are then continuously updated as additional samples are taken, and the observed values are compared against them to identify potential anomalies. However, the ADAPTIVE approach is implemented on longitudinal profiles of single markers (e.g., testosterone), of single combinations of two markers (e.g., testosterone over epitestosterone; the so-called T/E ratio), or, alternatively, of a few biomarkers following a univariate approach. Inspired by the use of copulas for modeling multivariate datasets, in this work we extend the established ADAPTIVE method to multivariate testing of longitudinal ABP profiles. We express the multivariate joint distribution of a set of biomarkers through a separate modeling of the marginal distributions and of their dependence structure. Focusing on a parametric setting, we evaluate the performance of such a multivariate approach in a number of simulation studies, varying according to the copula family and the number of biomarkers. Joint work with Brunero Liseo","tags":["Copula Models","Doping Detection","WADA"],"title":"Development of a multivariate Bayesian methodological framework for doping detection using copulae (Talk @ University of Cambridge)","type":"talk"},{"authors":["Xueqing Liu","Nina Deliu","Bibhas Chakraborty"],"categories":null,"content":" An interview related to this article has been made by the Editor-in-Chief of the American Journal of Public Health to Bibhas. Click the presentation button above to watch it.   ","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"3a401bf0210af231491aab9889d251fa","permalink":"https://nina-dl.github.io/publication/2023amph/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/publication/2023amph/","section":"publication","summary":"Just-in-time adaptive interventions (JITAIs) represent an intervention design that adapts the provision and type of support over time to an individual’s changing status and contexts, intending to deliver the right support on the right occasion. As a novel strategy for delivering mobile health interventions, JITAIs have the potential to improve access to quality care in underserved communities and, thus, alleviate health disparities, a significant public health concern. Valid experimental designs and analysis methods are required to inform the development of JITAIs. Here, we briefly review the cutting-edge design of microrandomized trials (MRTs), covering both the classical MRT design and its outcome-adaptive counterpart. Associated statistical challenges related to the design and analysis of MRTs are also discussed. Two case studies are provided to illustrate the aforementioned concepts and designs throughout the article. We hope our work leads to better design and application of JITAIs, advancing public health research and practice.","tags":["Microrandomized trials","Reinforcement learning","Public health"],"title":"Microrandomized Trials: Developing Just-in-Time Adaptive Interventions for Better Public Health","type":"publication"},{"authors":["Nina Deliu","Bibhas Chakraborty"],"categories":null,"content":"","date":1663632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663632000,"objectID":"6ee688b9b2ece3c8c05199b7377f723b","permalink":"https://nina-dl.github.io/publication/2022dtr_bookch/","publishdate":"2022-09-20T00:00:00Z","relpermalink":"/publication/2022dtr_bookch/","section":"publication","summary":"The study of evidence-based dynamic treatment regimes (DTRs) comprises an important line of methodological research within the domain of personalized medicine, a medical paradigm that transitions from the one-size-fits-all ideology. In this chapter we aim to provide an up-to-date comprehensive overview of this cutting-edge area of research, including its fundamental mathematical framework, the proposed methodologies for developing DTRs (to which a major focus is dedicated), existing data sources, the issue of inference, and other practical considerations. We enrich our discussion with illustration of recent real-life examples. We also highlight that, despite the increasing progress and interest in the topic, documented by remarkable theoretical results, their application in real life is still quite limited. We hope to endow the reader with a foundation for further study of the expansive DTRs literature, acknowledging that there is an enormous research-practice gap to be bridged.","tags":["Dynamic treatment regimes","Reinforcement learning"],"title":"Dynamic Treatment Regimes for Optimizing Healthcare","type":"publication"},{"authors":["Isabel Chien","Nina Deliu","Richard Turner","Adrian Weller","Sofia Villar","Niki Kilbertus"],"categories":null,"content":" Click the presentation button above to watch the talk related to this work, given at the FAccT \u0026lsquo;22 Conference.   ","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"6d62098e95c378b459d37a987ef452d8","permalink":"https://nina-dl.github.io/publication/2022fairness_facct/","publishdate":"2022-07-22T00:00:00Z","relpermalink":"/publication/2022fairness_facct/","section":"publication","summary":"While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.","tags":["Machine learning","Clinical trials","Adaptive designs"],"title":"Multi-disciplinary fairness considerations in machine learning for clinical trials","type":"publication"},{"authors":["Caroline A Figueroa","Nina Deliu","Bibhas Chakraborty","Arghavan Modiri","Jing Xu","Jai Aggarwal","Joseph Jay Williams","Courtney Lyles","Adrian Aguilera"],"categories":null,"content":"","date":1644537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644537600,"objectID":"5b3eccb5911d4f5e675e54770caca52c","permalink":"https://nina-dl.github.io/publication/2022diamante_abm/","publishdate":"2022-02-11T00:00:00Z","relpermalink":"/publication/2022diamante_abm/","section":"publication","summary":"Low physical activity is an important risk factor for common physical and mental disorders. Physical activity interventions delivered via smartphones can help users maintain and increase physical activity, but outcomes have been mixed. Here we assessed the effects of sending daily motivational and feedback text messages in a microrandomized clinical trial on changes in physical activity from one day to the next in a student population. We included 93 participants who used a physical activity app, [DIAMANTE](https://diamante.healthysms.org/) for a period of 6 weeks. Every day, their phone pedometer passively tracked participants’ steps. They were microrandomized to receive different types of motivational messages, based on a cognitive-behavioral framework, and feedback on their steps. We used generalized estimation equation models to test the effectiveness of feedback and motivational messages on changes in steps from one day to the next. Sending any versus no text message initially resulted in an increase in daily steps (729 steps, p = .012), but this effect decreased over time. A multivariate analysis evaluating each text message category separately showed that the initial positive effect was driven by the motivational messages though the effect was small and trend-wise significant (717 steps; p = .083), but not the feedback messages (−276 steps, p = .4). Sending motivational physical activity text messages based on a cognitive-behavioral framework may have a positive effect on increasing steps, but this decreases with time. Further work is needed to examine using personalization and contextualization to improve the efficacy of text-messaging interventions on physical activity outcomes.","tags":["Just-in-time adaptive interventions","Microrandomized trial","Thompson sampling","Reinforcement learning"],"title":"Daily Motivational Text Messages to Promote Physical Activity in University Students: Results From a Microrandomized Trial","type":"publication"},{"authors":["Nina Deliu"],"categories":null,"content":"","date":1636502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636502400,"objectID":"705ac96b3c4d2e27d9b43a9dfb4aba02","permalink":"https://nina-dl.github.io/talk/2021_armitage/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/talk/2021_armitage/","section":"talk","summary":"","tags":[],"title":"Design-based Inference in Response-Adaptive Randomization with Application to Thompson Sampling (Invited Talk @ 18th Armitage Workshop)","type":"talk"},{"authors":[],"categories":null,"content":"","date":1626480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626480000,"objectID":"38a7820eda834d8a7052d1d59fe69a94","permalink":"https://nina-dl.github.io/talk/2021_ysm/","publishdate":"2021-07-17T00:00:00Z","relpermalink":"/talk/2021_ysm/","section":"talk","summary":"I am delighted to announce that I was invited to give a talk at the Young Researchers Meeting, Bernoulli-IMS 10th WC in Probability and Statistics.","tags":[],"title":"Using reinforcement learning to design just-in-time adaptive interventions. The DIAMANTE pilot study (Invited Talk @ Young Researchers Meeting, Bernoulli-IMS2021)","type":"talk"},{"authors":["Caroline A Figueroa","Adrian Aguilera","Bibhas Chakraborty","Arghavan Modiri","Jai Aggarwal","Nina Deliu","Urmimala Sarkar","Joseph Jay Williams","Courtney R Lyles"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"63c56e88bcd593661ec816bb728ef36d","permalink":"https://nina-dl.github.io/publication/2021diamante_jamia/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/2021diamante_jamia/","section":"publication","summary":"Providing behavioral health interventions via smartphones allows these interventions to be adapted to the changing behavior, preferences, and needs of individuals. This can be achieved through reinforcement learning (RL), a sub-area of machine learning. However, many challenges could affect the effectiveness of these algorithms in the real world. We provide guidelines for decision-making. Using thematic analysis, we describe challenges, considerations, and solutions for algorithm design decisions in a collaboration between health services researchers, clinicians, and data scientists. We use the design process of an RL algorithm for a mobile health study “DIAMANTE” for increasing physical activity in underserved patients with diabetes and depression. Over the 1.5-year project, we kept track of the research process using collaborative cloud Google Documents, Whatsapp messenger, and video teleconferencing. We discussed, categorized, and coded critical challenges. We grouped challenges to create thematic topic process domains. Nine challenges emerged, which we divided into 3 major themes, i.e., 1. Choosing the model for decision-making, including appropriate contextual and reward variables; 2. Data handling/collection, such as how to deal with missing or incorrect data in real-time; 3. Weighing the algorithm performance vs effectiveness/implementation in real-world settings. The creation of effective behavioral health interventions does not depend only on final algorithm performance. Many decisions in the real world are necessary to formulate the design of problem parameters to which an algorithm is applied. Researchers must document and evaulate these considerations and decisions before and during the intervention period, to increase transparency, accountability, and reproducibility. Find the full eprint at the following [link](https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocab001/6154382?guestAccessKey=d11a3625-6403-442d-8729-5b31a9238ae0).","tags":["Reinforcement learning","Microrandomized trials"],"title":"Adaptive learning algorithms to optimize mobile applications for behavioral health: guidelines for design decisions","type":"publication"},{"authors":null,"categories":["machine learning","tidymodels"],"content":"  1 A sad script symphony 🎻 🎷 🎹 2 Packages 3 Data 4 Penguins 5 tidymodels 101 6 Hey Jude, don’t make it sad 🎶 7 Make it better   A few years ago, I did a talk called “Take a Sad Plot \u0026amp; Make it Better,” where I showed how I took a single sad plot and tried to make it better. The process of making that plot better taught me a lot about data visualization, and about the ggplot2 package.\nFast-forward to 2019 when I started learning tidymodels, and I have accumulated some pretty sad predictive modeling scripts! And my sad plots are not so lonely anymore. Specifically, my old scripts for doing cross-validation with tidymodels are particularly sad. But, I’ve been able to make them better (one might even call them happy), primarily due to changes in the tune package and the addition of the fit_resamples() function. The process of making these scripts better taught me a lot about predictive modeling, and about the (evolving) tidymodels ecosystem. So, why write a blog post with outdated code?\nI want to remember that I did this “by hand.” I want to remember how I did this “by hand.” The code still works, even if there is now a happier path to doing the same thing. I want to share cute penguin art and gifs.  Let’s start with some cute penguin art by Rohan Chakravarty…\n\nMy objective here is not to provide an introduction to using tidymodels, cross-validation, or to machine learning. If that is what you came for, check out the project button at the top of this post for my workshop materials for learners, and my associated blog post on the RStudio education site.\n Bottom line: If you are stumbling upon this blog post in the year 2020 or beyond, know that there is a better way!   1 A sad script symphony 🎻 🎷 🎹 I’m not the first person to write sad tidymodels scripts- there are many out in the wild. Here were the blog posts that I found most helpful when trying to solve this particular coding conundrum:\nModelling with Tidymodels and Parsnip: A Tidy Approach to a Classification Problem by Diego Usai\n A tutorial on tidy cross-validation with R by Bruno Rodrigues\n Modeling with parsnip and tidymodels by Benjamin Sorensen\n   2 Packages library(tidyverse) library(tidymodels) library(rpart) # for decision tree library(ranger) # for random forest  3 Data I’m going to use data that Allison Horst helped me source on penguins from the Palmer Station (Antarctica) Long Term Ecological Research Network.\n “sooo now I’m just looking at penguin pictures” - Allison Horst after slacking me this penguin data\n Here are the three dataset sources:\n Adelie penguins: https://portal.lternet.edu/nis/mapbrowse?packageid=knb-lter-pal.219.3 Chinstrap penguins: https://portal.lternet.edu/nis/mapbrowse?packageid=knb-lter-pal.220.3 Gentoo penguins: https://portal.lternet.edu/nis/mapbrowse?packageid=knb-lter-pal.221.2  I downloaded and imported these three datasets using R, then did some very light data wrangling and merged them into one called penguins, which I’ll use now:\npenguins \u0026lt;- read_csv( here::here(\u0026quot;content/post/2020-02-27-better-tidymodels/data/penguins.csv\u0026quot;)) %\u0026gt;% mutate_if(is.character, as.factor) #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; species = col_character(), #\u0026gt; culmen_length_mm = col_double(), #\u0026gt; culmen_depth_mm = col_double(), #\u0026gt; flipper_length_mm = col_double(), #\u0026gt; body_mass_g = col_double(), #\u0026gt; sex = col_character() #\u0026gt; ) glimpse(penguins) #\u0026gt; Observations: 333 #\u0026gt; Variables: 6 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ade… #\u0026gt; $ culmen_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.… #\u0026gt; $ culmen_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.… #\u0026gt; $ flipper_length_mm \u0026lt;dbl\u0026gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 1… #\u0026gt; $ body_mass_g \u0026lt;dbl\u0026gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 380… #\u0026gt; $ sex \u0026lt;fct\u0026gt; MALE, FEMALE, FEMALE, FEMALE, MALE, FEMALE, MALE, F…  4 Penguins This data included structural size measurements of penguins like their bill length, flipper length, and body mass. It also included each penguin’s species and sex. I’m going to use this data to try to predict penguin body mass. Sadly, we only have data for three distinct penguin species:\npenguins %\u0026gt;% count(species) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie 146 #\u0026gt; 2 Chinstrap 68 #\u0026gt; 3 Gentoo 119 Here is a lineup:\nFrom: https://www.bas.ac.uk/about/antarctica/wildlife/penguins/\nLooks like we have data for 3 of the smaller penguin species (of those pictured here).\nFirst, let’s build a simple linear regression model to predict body mass from flipper length.\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + geom_point(color = \u0026quot;salmon\u0026quot;, size = 3, alpha = .9) + geom_smooth(method = \u0026quot;lm\u0026quot;) + theme_penguin() Not bad! Looks promising. To actually fit a linear regression model, you might be used to something like this in R:\npenguin_mod \u0026lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins) summary(penguin_mod) #\u0026gt; #\u0026gt; Call: #\u0026gt; lm(formula = body_mass_g ~ flipper_length_mm, data = penguins) #\u0026gt; #\u0026gt; Residuals: #\u0026gt; Min 1Q Median 3Q Max #\u0026gt; -1057.33 -259.79 -12.24 242.97 1293.89 #\u0026gt; #\u0026gt; Coefficients: #\u0026gt; Estimate Std. Error t value Pr(\u0026gt;|t|) #\u0026gt; (Intercept) -5872.09 310.29 -18.93 \u0026lt;2e-16 *** #\u0026gt; flipper_length_mm 50.15 1.54 32.56 \u0026lt;2e-16 *** #\u0026gt; --- #\u0026gt; Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 #\u0026gt; #\u0026gt; Residual standard error: 393.3 on 331 degrees of freedom #\u0026gt; Multiple R-squared: 0.7621, Adjusted R-squared: 0.7614 #\u0026gt; F-statistic: 1060 on 1 and 331 DF, p-value: \u0026lt; 2.2e-16 But we aren’t going to stick with this. We are going to use tidymodels, with the goal of generating accurate predictions for future, yet-to-be-seen penguins.\n 5 tidymodels 101 The code provided in the section below is not particularly sad 🐧. If you are embarking on learning tidymodels, you’ll need to use this same kind of code as the building blocks for any predictive modeling pipeline.\n5.1 Parsnip: build the model This step is really three, using only the parsnip package:\nlm_spec \u0026lt;- linear_reg() %\u0026gt;% # pick model set_engine(\u0026quot;lm\u0026quot;) %\u0026gt;% # set engine set_mode(\u0026quot;regression\u0026quot;) # set mode lm_spec #\u0026gt; Linear Regression Model Specification (regression) #\u0026gt; #\u0026gt; Computational engine: lm Things that are missing: data (we haven’t touched it yet) and a formula (no data, no variables, no twiddle ~). This is an abstract model specification. See other possible parsnip models here.\n 5.2 Recipe: not happening here, folks This is where you would normally insert some code for feature engineering using the recipes package. But previously this required functions named prep(), bake(), juice()- so I’m willfully ignoring that for now. There will be no recipes involving penguins.\n 5.3 Rsample: initial split We’ll use the rsample package to split (ayee! I promise no penguins were hurt in the writing of this blog post) the penguins up into two datasets: training and testing. If you are unfamiliar with this practice, read up on the holdout method.\npenguin_split \u0026lt;- initial_split(penguins, strata = species) penguin_train \u0026lt;- training(penguin_split) penguin_test \u0026lt;- testing(penguin_split)  5.4 Fitting the model once Fitting a single model once is…not exactly the hardest part.\nThis is essentially the workflow from this early blog post.\nset.seed(0) lm_spec %\u0026gt;% # train: get fitted model fit(body_mass_g ~ ., data = penguin_train) %\u0026gt;% # test: get predictions predict(new_data = penguin_test) %\u0026gt;% # compare: get metrics bind_cols(penguin_test) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 297.  5.5 Fitting the model with a function If you squint, you might see that I could make this into a function like below:\nget_rmse \u0026lt;- function(model_spec, split) { model_spec %\u0026gt;% # train: get fitted model fit(body_mass_g ~ ., data = training(split)) %\u0026gt;% # test: get predictions predict(new_data = testing(split)) %\u0026gt;% # compare: get metrics bind_cols(testing(split)) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) } And I could use it to fit a linear regression model:\nset.seed(0) get_rmse(model_spec = lm_spec, split = penguin_split) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 297. I could also build up a tibble that includes the results, if I wanted to save the predicted values, for example:\nget_preds \u0026lt;- function(model_spec, split){ # train: get fitted model fit_model \u0026lt;- model_spec %\u0026gt;% fit(body_mass_g ~ ., data = training(split)) # test: get predictions preds \u0026lt;- fit_model %\u0026gt;% predict(new_data = testing(split)) %\u0026gt;% bind_cols(testing(split) %\u0026gt;% select(body_mass_g, species)) preds } set.seed(0) penguin_preds \u0026lt;- get_preds(model_spec = lm_spec, split = penguin_split) Then I can work with the predicted values, like plotting the fitted body mass estimates against the residuals.\nggplot(penguin_preds, aes(x = .pred, y = (.pred - body_mass_g))) + geom_point(aes(colour = species), size = 3, alpha = .8) + geom_smooth(method = \u0026quot;lm\u0026quot;) + theme_penguin() + scico::scale_colour_scico_d(end = .8) + ggtitle(\u0026quot;Residuals vs Fitted\u0026quot;) #\u0026gt; `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;  # compare: get metrics penguin_preds %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 297. Or I could fit a regression tree model with a new model spec:\n# regression tree model spec rt_spec \u0026lt;- decision_tree() %\u0026gt;% set_engine(\u0026quot;rpart\u0026quot;) %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;) # get rmse set.seed(0) get_preds(model_spec = rt_spec, split = penguin_split) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 321. Or a random forest:\n# random forest model spec rf_spec \u0026lt;- rand_forest() %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;) # get rmse set.seed(0) get_preds(model_spec = rf_spec, split = penguin_split) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 299. But, unfortunately, I shouldn’t be predicting with the test set over and over again like this. It isn’t good practice to predict with the test set \u0026gt; 1 time. What is a good predictive modeler to do? I should be saving (holding out) the test set and use it to generate predictions exactly once, at the very end — after I’ve compared different models, selected my features, and tuned my hyperparameters. How do you do this? You do cross-validation with the training set, and you leave the testing set for the very last fit you do.\n  6 Hey Jude, don’t make it sad 🎶 Now, for the 😭 part- let’s add cross-validation! To do this, we’ll use a function called rsample::vfold_cv().\n# add the cv step here set.seed(0) penguin_folds \u0026lt;- vfold_cv(data = penguin_train, strata = \u0026quot;species\u0026quot;) penguin_folds #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 2 #\u0026gt; splits id #\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 The process of training, testing, and computing metrics gets a lot harder when you need to do this across 10 folds, each with a different data split. I eventually worked out three approaches, which I show below. All require some level of comfort with iteration using the purrr package.\n6.1 Function with minimal purrr-ing This approach is essentially a mega-function, that we then use purrr to map across each fold.\nI’m going to change a few things from my previous get_preds() function:\ntraining(split) -\u0026gt; analysis(split) testing(split) -\u0026gt; assessment(split) I also added the rsample::add_resample_id() function to keep track of the fold number. I saved the predictions now as a list column.  To build up this function, my strategy was to figure out how to work with one fold, then I knew I’d be able to use purrr::map_df() to apply it across multiple folds.\n# Figure it out for one fold get_fold_results \u0026lt;- function(model_spec, split){ # train: get fitted model for each fold fits \u0026lt;- model_spec %\u0026gt;% fit(body_mass_g ~ ., data = analysis(split)) # test: get predictions on for each fold preds \u0026lt;- fits %\u0026gt;% predict(new_data = assessment(split)) %\u0026gt;% bind_cols(assessment(split)) # compare: compute metric for each fold rmse \u0026lt;- assessment(split) %\u0026gt;% summarize(rmse = rmse_vec(truth = body_mass_g, estimate = preds$.pred)) rmse %\u0026gt;% # add fold identifier column rsample::add_resample_id(split = split) %\u0026gt;% as_tibble() %\u0026gt;% # add predictions mutate(preds = list(preds)) } I tried this function with a single fold first:\nset.seed(0) get_fold_results( split = penguin_folds$splits[[1]], model_spec = rt_spec ) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; rmse id preds #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 291. Fold01 \u0026lt;tibble [26 × 7]\u0026gt; Next, I used purrr- but just once. The function get_fold_results is doing most of the work for us, but I needed purrr to map it across each fold.\nset.seed(0) kfold_results \u0026lt;- map_df( penguin_folds$splits, ~get_fold_results(.x, model = rt_spec)) kfold_results #\u0026gt; # A tibble: 10 x 3 #\u0026gt; rmse id preds #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 291. Fold01 \u0026lt;tibble [26 × 7]\u0026gt; #\u0026gt; 2 298. Fold02 \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 3 303. Fold03 \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 4 359. Fold04 \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 5 320. Fold05 \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 6 434. Fold06 \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 7 320. Fold07 \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 8 245. Fold08 \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 9 262. Fold09 \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 10 343. Fold10 \u0026lt;tibble [25 × 7]\u0026gt; Here we are still left with 10 RMSE values- one for each of the 10 folds. We don’t care too much about by fold- the power is in the aggregate. Specifically, we mainly care about the central tendency and spread of these RMSE values. Let’s finish by combining (or aggregating) these metrics.\nkfold_results %\u0026gt;% summarize(mean_rmse = mean(rmse), sd_rmse = sd(rmse)) #\u0026gt; # A tibble: 1 x 2 #\u0026gt; mean_rmse sd_rmse #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 317. 53.4 So, this works. But, can you imagine doing it again? Without errors? Can you imagine teaching it?\n 6.2 Purrr-to-the-max This approach is purrr::map() (and friends) on steriods. We use vanilla map(), map2(), and map2_dbl() here. We also use anonymous functions as a formula, and the pipe operator within those anonymous functions.\nset.seed(0) penguin_res \u0026lt;- penguin_folds %\u0026gt;% mutate( # train: get fitted model for each fold train_set = map(splits, analysis), fit_models = map(train_set, ~rt_spec %\u0026gt;% fit(body_mass_g ~ ., data = .x)), # test: get predictions for each fold test_set = map(splits, assessment), estimates = map2(fit_models, test_set, ~.x %\u0026gt;% predict(.y)), # compare: compute metric for each fold rmse = map2_dbl(test_set, estimates, ~rmse_vec(truth = .x$body_mass_g, estimate = .y$.pred)) ) penguin_res #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 7 #\u0026gt; splits id train_set fit_models test_set estimates rmse #\u0026gt; * \u0026lt;named lis\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named lis\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 \u0026lt;split [22… Fold01 \u0026lt;tibble [225 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26… \u0026lt;tibble [26… 291. #\u0026gt; 2 \u0026lt;split [22… Fold02 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 298. #\u0026gt; 3 \u0026lt;split [22… Fold03 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 303. #\u0026gt; 4 \u0026lt;split [22… Fold04 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 359. #\u0026gt; 5 \u0026lt;split [22… Fold05 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 320. #\u0026gt; 6 \u0026lt;split [22… Fold06 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 434. #\u0026gt; 7 \u0026lt;split [22… Fold07 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 320. #\u0026gt; 8 \u0026lt;split [22… Fold08 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 245. #\u0026gt; 9 \u0026lt;split [22… Fold09 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 262. #\u0026gt; 10 \u0026lt;split [22… Fold10 \u0026lt;tibble [226 … \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25… \u0026lt;tibble [25… 343. penguin_res %\u0026gt;% summarise(mean_rmse = mean(rmse), sd_rmse = sd(rmse)) #\u0026gt; # A tibble: 1 x 2 #\u0026gt; mean_rmse sd_rmse #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 317. 53.4  6.3 The purrr mash-up Another way I worked out was largely after reviewing Max’s slides from previous workshops. This is basically a mash-up of my previous two approaches, where we write laser-focused functions that each do one thing, then use purrr to apply those functions across the folds. This way is nice(r) for showing in slides as you can incrementally build up the results table. Let’s see this sad script in action…\n6.3.1 Round 1 set.seed(0) # for reproducibility # train: get fitted model for a split get_fits \u0026lt;- function(split, model_spec){ model_spec %\u0026gt;% fit(body_mass_g ~ ., data = analysis(split)) } # train: get fitted models across folds penguin_purrr \u0026lt;- penguin_folds %\u0026gt;% mutate(rt_fits = map(splits, get_fits, rt_spec)) penguin_purrr #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 3 #\u0026gt; splits id rt_fits #\u0026gt; * \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt;  6.3.2 Round 2 # test: get predictions for a split get_preds \u0026lt;- function(split, fit_df) { fit_df %\u0026gt;% predict(new_data = assessment(split)) %\u0026gt;% bind_cols(assessment(split)) } # test: get predictions across folds penguin_purrr \u0026lt;- penguin_purrr %\u0026gt;% mutate(rt_preds = map2(splits, rt_fits, get_preds)) penguin_purrr #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 4 #\u0026gt; splits id rt_fits rt_preds #\u0026gt; * \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 × 7]\u0026gt; #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt;  6.3.3 aaaand Round 3 # compare: compute metric for a split get_rmse \u0026lt;- function(pred_df) { pred_df %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) %\u0026gt;% pluck(\u0026quot;.estimate\u0026quot;) } # compare: compute metric across folds penguin_purrr \u0026lt;- penguin_purrr %\u0026gt;% mutate(rt_rmse = map_dbl(rt_preds, get_rmse)) penguin_purrr #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 5 #\u0026gt; splits id rt_fits rt_preds rt_rmse #\u0026gt; * \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 × 7]\u0026gt; 291. #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 298. #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 303. #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 359. #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 320. #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 434. #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 320. #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 245. #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 262. #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 343. Finally, summarizing as I did before:\npenguin_purrr %\u0026gt;% summarize(mean_rmse = mean(rt_rmse), sd_rmse = sd(rt_rmse)) #\u0026gt; # A tibble: 1 x 2 #\u0026gt; mean_rmse sd_rmse #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 317. 53.4 In practice, if you did all these at once instead of incrementally, it would look like:\nset.seed(0) penguin_folds %\u0026gt;% # train: get fitted model for a split mutate(rt_fits = map(splits, get_fits, rt_spec)) %\u0026gt;% # test: get predictions on for each fold mutate(rt_preds = map2(splits, rt_fits, get_preds)) %\u0026gt;% # compare: compute metric for each fold mutate(rt_rmse = map_dbl(rt_preds, get_rmse)) #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 5 #\u0026gt; splits id rt_fits rt_preds rt_rmse #\u0026gt; * \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 × 7]\u0026gt; 291. #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 298. #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 303. #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 359. #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 320. #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 434. #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 320. #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 245. #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 262. #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 × 7]\u0026gt; 343. When you put it like that, it doesn’t look like so much work! But, this way hides how much work it takes to write those 3 custom functions: get_fits(), get_preds(), and get_rmse(). And we still had to use vanilla map(), map2(), and map2_dbl().\n   7 Make it better I kept a learning log while working through the all the above code, and I wrote down these notes to myself:\nIt is very easy to do the wrong thing; it is very hard to do the right thing.\n I lost sight many times of what the code I was writing was doing, because I was using up so much cognitive energy on getting the code to just work.\n I thought I knew how to use purrr…\n  If you have made it this far, I’m pretty sure I don’t need to convince you that a better way to do cross-validation using tidymodels would be more pleasant to do more than once. It would also be less prone to error due to me copying-and-pasting repeatedly, and making stupid mistakes that would be difficult to spot with so much cluttered code. Luckily, tune::fit_resamples() came along to take a sad script and make it better:\npenguin_party \u0026lt;- tune::fit_resamples( body_mass_g ~ ., model = rt_spec, resamples = penguin_folds ) Here is the beautiful output from that function:\npenguin_party #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 4 #\u0026gt; splits id .metrics .notes #\u0026gt; * \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;tibble [2 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; Now, to see all the stuff inside this penguin_party, we can use tune’s collect_* functions.\npenguin_party %\u0026gt;% collect_metrics() #\u0026gt; # A tibble: 2 x 5 #\u0026gt; .metric .estimator mean n std_err #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 317. 10 16.9 #\u0026gt; 2 rsq standard 0.827 10 0.0303 To see the predictions, we need to add use control_resamples():\npenguin_party \u0026lt;- tune::fit_resamples( body_mass_g ~ ., model = rt_spec, resamples = penguin_folds, control = control_resamples(save_pred = TRUE) # add this line ) Then we collect the predictions.\npenguin_party %\u0026gt;% collect_predictions() #\u0026gt; # A tibble: 251 x 4 #\u0026gt; id .pred .row body_mass_g #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Fold01 4040. 4 4675 #\u0026gt; 2 Fold01 3428. 11 3800 #\u0026gt; 3 Fold01 3428. 14 3200 #\u0026gt; 4 Fold01 4040. 41 3750 #\u0026gt; 5 Fold01 4040. 54 3900 #\u0026gt; 6 Fold01 3428. 65 3400 #\u0026gt; 7 Fold01 3428. 70 2900 #\u0026gt; 8 Fold01 4040. 71 4100 #\u0026gt; 9 Fold01 3428. 76 2925 #\u0026gt; 10 Fold01 4040. 85 3775 #\u0026gt; # … with 241 more rows Now, isn’t that better?\n ","date":1582761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582761600,"objectID":"6b69e5ca6732b982a2d9b7db27a31414","permalink":"https://nina-dl.github.io/post/2020-02-27-better-tidymodels/","publishdate":"2020-02-27T00:00:00Z","relpermalink":"/post/2020-02-27-better-tidymodels/","section":"post","summary":"Taking a sad script and making it better for model cross-validation.","tags":["tidymodels"],"title":"Take a Sad Script \u0026 Make it Better: Tidymodels Edition","type":"post"},{"authors":[],"categories":[],"content":" I’m excited to be teaching a new workshop at the upcoming rstudio::conf in January called “Introduction to Machine Learning with the Tidyverse”, with my colleague Garrett Grolemund. Our workshop just sold out over the weekend! 🎉\nIt is always hard to develop an entirely new workshop, especially if you are doing it at the same time as learning how to use a new API. It is even harder when that API is under active development like the tidymodels ecosystem! I’ve been so lucky to be able to work with the tidymodels team at RStudio, Max Kuhn and Davis Vaughan, to help shape how we tell the tidymodels story to ML beginners. But my favorite part of developing a new workshop like this has been studying how others teach machine learning. Spoiler alert: there are a lot of materials intended for learners that make things seem harder than they actually are! Below, I’m sharing my bookmarked resources, organized roughly in the order I think they are most helpful for beginners.\nMachine Learning for Everyone. In simple words. With real-world examples. Yes, again. In my experience, the biggest hurdle to getting started is sifting through both the hype and the math. This is a readable illustrated introduction to key concepts that will help you start building your own mental model of this space. For example, “the only goal of machine learning is to predict results based on incoming data. That’s it.” There you go! Start here.\n\n A Visual Introduction to Machine Learning by r2d3. This is a wonderful two-part series (that I wish would be extended!):\n Part I: A Decision Tree Part II: Model Tuning and the Bias-Variance Tradeoff   Supervised Machine Learning course by Julia Silge Taught with R and the caret package (the precursor to the in-development tidymodels ecosystem), this is a great next step in your machine learning journey as you’ll start doing ML right away in your browser using an innovative course delivery platform. You’ll also get to play with data that is not iris, titanic, or AmesHousing. This will be sweet relief because you’ll find the rest of my recommended resources all basically build models to predict home prices in Ames, Iowa.\n\n Hands-on Machine Learning with R by Bradley Boehmke \u0026amp; Brandon Greenwell. Another great way to learn concepts plus code, although another one that focuses on the caret package (pre-tidymodels). Each chapter maps onto a new learning algorithm, and provides a code-through with real data from building to tuning. The authors also offer practical advice for each algorithm, and the “final thoughts” sections at the end of each chapter will help you tie it all together.\n\nDon’t skip the “Fundamentals” section, even if you feel like you’ve got that down by now. The second chapter on the modeling process is especially good.\n\n Interpretable Machine Learning: A Guide for Making Black Box Models Explainable by Christoph Molnar. If you only have time to read a single chapter, skip ahead to Chapter 4: Interpretable Models. I also appreciated the introduction section on terminology. But the whole book is excellent and well-written.\n\n Model evaluation, model selection, and algorithm selection in machine learning- a 4-part series by Sebastian Raschka. I found this to be a great evidence-based, thorough overview of the methods for machine learning. I especially liked how he walks you step-by-step from the simplest methods like the holdout method up to nested cross-validation:\n Part I: The Basics Part II: Bootstrapping \u0026amp; uncertainties Part III: Cross-validation and hyperparameter tuning Part IV: Comparing the performance of machine learning models and algorithms using statistical tests and nested cross-validation   At this point, if you can read through the above resources and you are no longer feeling awash in new terminology, I think your vocabulary and mental model are in pretty good shape! That means you are ready for the next step, which is to read Max Kuhn and Kjell Johnson’s new book Feature Engineering and Selection: A Practical Approach for Predictive Models\n\nIn my experience, the later chapters in this book filled in a lot of lingering questions I had about certain methods, like whether to use factor or dummy variables in tree-based models. But also don’t miss the section on “important concepts” at the beginning- this should feel like a nice review if you’ve gotten this far!\n Elements of Statistical Learning. The entire PDF of the book is available online. A great resource for those with a strong statistics background, and for those looking for more math and formulas.\n  Other note-worthy resources  For the highly visual learner, you may want to cue up some YouTube videos from Udacity’s “Machine Learning for Trading” course. I found these illustrations especially helpful:\n Cross-validation Overfitting Ensemble learners Bootstrap aggregating (bagging) Boosting   Chris Albon’s Machine Learning Flashcards ($12)\n Shirin Elsinghorst’s blog (free! and so good).\n\nI love her sketchnotes.\n I also found Rafael Irizarry’s “Introduction to Machine Learning”, a chapter from his Introduction to Data Science book, to have some helpful discussion.\n Machine Learning: A primer by Danilo Bzdok, Martin Krzywinski \u0026amp; Naomi Altman, from the Nature Methods Points of Significance collection- this collection in general is always straight-forward with great visuals. Start with the primer, then skim these:\n Statistics versus machine learning Machine learning: supervised methods Classification and regression trees (decision trees are the “base learner” for many ensemble methods - this is a good intro) Ensemble methods: bagging and random forests    That’s all for now- if you are taking my workshop in January I look forward to meeting you in person! If not, rest assured that all code and materials will be shared openly after the workshop. Until then, happy learning 🤖\n ","date":1577059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119901,"objectID":"416351d525a4abd88ecf05ec88076d80","permalink":"https://nina-dl.github.io/post/2019-12-23-learning-to-teach-machines-to-learn/","publishdate":"2019-12-23T00:00:00Z","relpermalink":"/post/2019-12-23-learning-to-teach-machines-to-learn/","section":"post","summary":"I’m excited to be teaching a new workshop at the upcoming rstudio::conf in January called “Introduction to Machine Learning with the Tidyverse”, with my colleague Garrett Grolemund. Our workshop just sold out over the weekend! 🎉\nIt is always hard to develop an entirely new workshop, especially if you are doing it at the same time as learning how to use a new API. It is even harder when that API is under active development like the tidymodels ecosystem!","tags":[],"title":"Learning to Teach Machines to Learn","type":"post"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"  #1: Update Hugo #2: Change the baseurl #3: Netlify drag-and-drop #4: Torch public/ #5: Peruse public/ #6: Back to the future    “Just a spoonful of Hugo helps the blog go down.” - me, only somewhat kidding\n In this series, I’m sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated). You can read the previous posts about my “Spoonful of Hugo” series about Hugo archetypes, Hugo versions, and Hugo page bundles.\nThe following are a few steps that I always start with to troubleshoot any blogdown/Hugo/Netlify problems. These steps would solve what I would anecdotally estimate as ~50% of blogdown problems that I see posted in the GitHub repository and on the community site.\n#1: Update Hugo  Figure 1: Don’t be like this  If things have gone south and you are getting Hugo errors when you use the “Serve Site” Addin locally, it is possible that you need to update your version of Hugo. From R, you can check your Hugo version with blogdown:\nblogdown::hugo_version() Then you can reference your Hugo theme to find the minimum version of Hugo required by your theme:\n Figure 2: Check your theme’s minimum Hugo version  You can go higher than the minimum version though, so it’s good practice to update your Hugo, again from within R:\nblogdown:: update_hugo() Check your version again post-update:\nblogdown::hugo_version() ## [1] \u0026#39;0.55.6\u0026#39; If you are using Netlify to build your site using Hugo, you’ll want this version to match that- the best way to do that is with a netlify.toml file.\n #2: Change the baseurl Open up your config.toml file and look for the baseurl field, usually pretty close to the top. Here is mine1:\nbaseurl = \u0026quot;https://alison.rbind.io\u0026quot; Now if you are just starting with Hugo and don’t actually have a domain name yet, try taking the advice that blogdown automatically prints out for you:\nWarning: You should change the \u0026quot;baseurl\u0026quot; option in config.toml from https://example.org to your actual domain; if you do not have a domain, set \u0026quot;baseurl\u0026quot; to \u0026quot;/\u0026quot; But be careful here- you shouldn’t leave it as “/”- once you do have your domain name you should update the baseurl as “/” is a not a valid URL.\nCare to know more? Here is a quote from the person who writes the Hugo docs:\n “…the only purpose for the baseurl field in the config is to define the full base URL of your website for deployment purposes.” - @rdwatters\n The main error that would happen without the trailing slash in the past is that you would end up with a site where the theme’s CSS would be all wrong. This was probably because the theme designer used code like this buried in a layout file:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ .Site.BaseURL }}css/style.css\u0026quot;/\u0026gt; Now, if you set baseurl = \u0026quot;http://mysite.com\u0026quot; but only rendered locally, things would look just peachy, because the default local server already included the trailing slash. So, the link in the html file would be2:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;http://localhost:1313/css/style.css\u0026quot;\u0026gt; But, at build, the link in the html file would turn into:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;http://mysite.comcss/style.css\u0026quot;\u0026gt; Which creates sites that look like this:\n Figure 3: Hugo tranquil peak theme  GitHub issue #369\n Figure 4: Hugo universal theme  GitHub issue #131\nGitHub issue #114\nHowever, Hugo authors and theme developers have largely been moving towards using relative URLs instead of the baseurl to build paths. This was based on public advice voiced by the Hugo authors on the discourse forum. For example:\n “The recommended way to reference resources is to use either relURL or absURL template funcs, which handles the slash issues.”- @bep\n Following that advice, a more up-to-date theme would have code that looks like this buried in a layout file:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ \u0026quot;css/style.css\u0026quot; | relURL }}\u0026quot;/\u0026gt; →  Bottom line? If your theme uses relURL or absURL to link to site resources like CSS, JavaScript, or static images, then whether or not you include a trailing slash in your baseurl should not matter at all.\nAnd here is some tough love about your theme: if the most recent version does still require the trailing slash in the baseurl to “work” out of the box, I would seriously consider switching themes. This is a pretty good “canary in the coal mine” test regarding how up-to-date the theme author is, and how well the theme you have chosen adheres to Hugo templating best practices. If you are having pain with this now, it is likely not the only thing that will be painful about working with your theme.\n  #3: Netlify drag-and-drop If you can render your site locally but your published site looks different, try the drag-and-drop method:\nUse the “Serve Site” Addin, then drag-and-drop the public/ folder straight into Netlify. What does this do? You can now see your public site…that you built…with your local version of Hugo. Netlify is doing none of the site building here.\nOne of the first benefits of this approach is that it ensures that you are able to actually generate a public/ folder locally! I have seen folks struggle to deploy the wrong repo. This simple step can force you to make sure to use the “Serve Site” Addin to generate the public/ folder, and that the repo you are trying to link to Netlify actually contains a Hugo site because you must physically move the public/ folder. But this method can also help you diagnose other problems too.\nIf your public/ folder does not render on Netlify, you have work to do locally. I can’t tell you what it is as it can be a number of things, but you can be sure that your problem is not just the Netlify build- it is your local build too.\nIf your public/ folder does render perfectly on Netlify, but you are getting a Netlify build error, then you likely have a Hugo version problem. It might be that the version you are running locally is more recent than the version run by Netlify by default to actually build your site. The good news is there is a quick fix for this! The solution is to upgrade the Hugo version Netlify is using- see my advice here for how to do that.\nIf you are happy with how your site looks but you are missing content and/or seeing old deleted content, then you may need the next few strategies to troubleshoot.\n #4: Torch public/ When you are seeing very weird things locally, try deleting your local public/ folder. Then serve site again. Sometimes it can get “junked up”. I’ve found that sometimes deleted content can be a little sticky. As recommended in the blogdown book:\n “you are strongly recommended to delete the /public/ directory before you rebuild the site for publishing every time, because Hugo never deletes it”\n Also, this has a bonus of reinforcing for you exactly what the “Serve Site” Addin does - it regenerates the public/ folder. This is also the folder that, if you are using Netlify to build your site, is in your .gitignore file because Netlify (+ Hugo) generates this file “fresh” with each push to your GitHub repository.\n #5: Peruse public/ When you notice weird things, try actually looking inside public/- don’t be afraid to spelunk around in there! If you are seeing something wrong with your site, try to figure out how blogdown/Hugo is processing and rendering your content. This folder can tell you a lot! Keep in mind that your local public/ folder will still contain future/draft/expired content if you used the “Serve Site” Addin.\n #6: Back to the future  Figure 5: Where are my posts?  If your site renders beautifully locally, and your drag-and-drop site from public/ looks the same, but you are missing key content when you actually deploy to Netlify using a Hugo build, you may have inadvertently stumbled into a Hugo date time warp. This is a fairly common gotcha. Try using the drag-and-drop method again, this time first delete public/, then instead of using the “Serve Site” Addin, run this in your console:\nblogdown::build_site(local = FALSE) Plop this new public folder in Netlify to see what your site will look like when it is actually published. What does this show you? Your local Hugo build (read: your public/ folder generated by “Serve Site”) differs by design in 3 important ways from your deployed site built by Netlify/Hugo. By default, Hugo will not publish:\n Content with a future publishDate value\n Content with draft: true status\n Content with a past expiryDate value\n  You can see that these are defaults. The behavior of the “Serve Site” Addin is also documented in the blogdown book:\n “This is for you to preview draft and future posts locally.”\n Blogdown’s build_site(local = FALSE) differs from the “Serve Site” Addin in that it will not render draft, future, or expired content. So your public/ folder from build_site(local = FALSE) shows you exactly what Netlify should publish. Seeing it can help you troubleshoot why some content was showing up locally but not when you publish.\nThe defaults are pretty sensible and nice to have, as you can still put these kinds of content under version control, and hence collaborate with other team members on the content without having the content publish (or expire) until you say so.\nTo show content that Hugo was hiding, you’ll want to edit some YAML fields in the individual offending content files. For example, in the YAML of an individual content file (like a blog post), if you want to un-draft it, add or change this key/value:\ntitle: \u0026#39;A Spoonful of Hugo: Troubleshooting your Build\u0026#39; author: \u0026quot;Alison Hill\u0026quot; date: \u0026#39;2019-03-04\u0026#39; draft: false Alternatively, if you want to date something in the future (like to advertise the date of an upcoming talk) but publish now, you can use the publishDate field. The publishDate field is a newer addition to Hugo (\u0026gt;= v0.54.0) which, if left unset, will default to the date field, which means in the individual content file YAML you can do:\ntitle: \u0026#39;A Spoonful of Hugo: Get excited!!\u0026#39; author: \u0026quot;Alison Hill\u0026quot; date: \u0026#39;2025-03-04\u0026#39; publishDate: \u0026#39;2019-03-04\u0026#39; Hopefully these 6 things can help you get unstuck. If not, the RStudio community forums are a great place to ask questions!\n  Yes that’s right, I don’t have a trailing slash- read on for why I can get away with this.↩\n https://discourse.gohugo.io/t/how-not-to-specify-url-site/5691/5↩\n   ","date":1551657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551657600,"objectID":"0ba13389195341f445a9a6be1766a7ec","permalink":"https://nina-dl.github.io/post/2019-03-04-hugo-troubleshooting/","publishdate":"2019-03-04T00:00:00Z","relpermalink":"/post/2019-03-04-hugo-troubleshooting/","section":"post","summary":"A few troubleshooting strategies to save your sanity","tags":["blogdown"],"title":"A Spoonful of Hugo: Troubleshooting Your Build","type":"post"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"  “Just a spoonful of Hugo helps the blog go down.” - me, only somewhat kidding\n In this series, I’m sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated). You can read the previous posts about my “Spoonful of Hugo” series about Hugo archetypes and Hugo versions.\nThis is my third post in this series and it is breaking news.\nHugo Page Bundles Well, not really breaking news, but you still may not know about it! Hugo v0.32 introduced a new feature called Page Bundles, as a way to organize the content files. Blogdown users rejoice that Davis Vaughn posted an issue on the rstudio/blogdown repo to enable this option, which Yihui added shortly before rstudio::conf 2019 🎉. Here is the snippet from the NEWS.md:\n “One benefit of using a page bundle instead of a normal page is that you can put resource files associated with the post (such as images) under the same directory of the post itself. This means you no longer have to put them under the static/ directory, which has been quite confusing to Hugo beginners.”\n What does a blogdown/Hugo site begin to look like without page bundles? I think here is a representative example from tidyverse.org (sorry tidyverse team- it’s not you, it’s the old Hugo).\nFor this team, they need an image for every post, which gets out of control pretty fast. Also, some ended up in static/ too, organized by post (which I have done on my own blog, though not well or consistently).\nWhat would it look like to use page bundles?\ncontent/ ├── about │ ├── index.md ├── posts │ ├── 2015-07-23-hi-world │ │ ├── bakers.csv │ │ ├── image1.jpg │ │ ├── image2.png │ │ └── index.Rmd │ └── 2015-07-24-bye-world │ └── index.Rmd One could call this bundled file structure “tidier” 🍱.\nIn the above, after serving site, index.html files also get added to the bundle. In Hugo’s terms, these are leaf bundles. The resource files allowed in a bundle include page and non-page items like images, pdf, .csv files, etc.\nThis is instead of:\ncontent/ ├── about │ ├── index.md ├── posts │ ├── 2015-07-23-hi-world.Rmd │ ├── bakers.csv │ ├── image1.jpg │ ├── image2.png │ └── 2015-07-24-bye-world.Rmd When you create a new bundled post, the actual content of the post goes in the index file of a page bundle. So:\n# not bundled post post/2015-07-23-hi-world.Rmd # bundled post post/2015-07-24-bye-world/index.Rmd  Bundle Me, blogdown! First, read the previous post on setting up a netlify.toml file. Since using Hugo page bundles depends on Hugo v0.32 or higher, you should go ahead and update hugo then update your netlify.toml with your updated version:\nblogdown::update_hugo() blogdown::hugo_version() Now, let’s use the usethis package.\nProject-specific .Rprofile First, I’m going to demo here how to create a project-specific .Rprofile file- but know that you can do a user-level .Rprofile file too.\n# install.packages(\u0026quot;usethis\u0026quot;) # uncomment this to install usethis::edit_r_profile(scope = \u0026quot;project\u0026quot;) These helpful messages should print to your console: please note the “restart” reminder…\n\u0026gt; usethis::edit_r_profile(scope = \u0026quot;project\u0026quot;) ● Restart R for changes to take effect ✔ Setting active project to \u0026#39;/Users/alison/rprojs/alison.rbind.io\u0026#39; ● Modify \u0026#39;.Rprofile\u0026#39; Now you could add this to your file:\n# in .Rprofile of the website project if (file.exists(\u0026quot;~/.Rprofile\u0026quot;)) { base::sys.source(\u0026quot;~/.Rprofile\u0026quot;, envir = environment()) } options(blogdown.new_bundle = TRUE) The first code chunk above is from the blogdown book, where we describe a workaround for loading both user and project .Rprofile files (since R technically only reads one startup profile file).\nIf you don’t want this, you could add the blogdown options to your user .Rprofile instead using:\nusethis::edit_r_profile(scope = \u0026quot;user\u0026quot;) Heck, while you are at it, you could set a bunch of options to make your blogdown life easier:\n# in .Rprofile of the website project if (file.exists(\u0026quot;~/.Rprofile\u0026quot;)) { base::sys.source(\u0026quot;~/.Rprofile\u0026quot;, envir = environment()) } options( blogdown.author = \u0026quot;Alison Hill\u0026quot;, blogdown.ext = \u0026quot;.Rmd\u0026quot;, blogdown.subdir = \u0026quot;post\u0026quot;, blogdown.yaml.empty = TRUE, blogdown.new_bundle = TRUE, blogdown.title_case = TRUE ) For the blogdown-specific options, any of these prepopulate content in your “New Post” Addin (I told you to use this here). There is a handy table from the blogdown book, summarized here:\n blogdown.author = author of new posts blogdown.ext = default extension of new posts (can also be “.md” or “.Rmarkdown”) blogdown.subdir = theme-specific, you need to know your theme and content folder here blogdown.yaml.empty = I told you to do that here blogdown.new_bundle = what this whole post is about! blogdown.title_case = “nEed More coFFee” –\u0026gt; “Need More Coffee” (it tidies all your post titles to title case)   The Newline Thing Here is a massive .Rprofile gotcha: this file must end with a blank line. So make sure you add an empty line at the end of the file, then save it, and restart your R session.\nWant to make your general R life easier in the future? Follow Yihui’s advice and do this in RStudio to ensure that all source files end with a newline:\n  Use Bundles After restarting R, try using the “New Post” Addin, this time with feeling. There is still one more gotcha though. Use the Addin to create your new bundled post. The only catch is that once you are looking at your exciting new post, you should delete the slug in the YAML (I posted an issue about this here).\nThe reason is that you want the link to your post to be:\nhttp://alison.rbind.io/post/2019-02-21-hugo-page-bundles/\nIf you include the slug, the link to your post will be:\nhttp://alison.rbind.io/post/2019-02-21-hugo-page-bundles/hugo-page-bundles\nAnother option is to update your config.toml file with permalinks like Yihui suggests (but beware: this will change all your past links as well, requiring some Netlify redirects):\n[permalinks] post = \u0026quot;/:year/:month/:day/:slug/\u0026quot; The default here from Hugo was /post/:year-:month-:day-:slug/:slug/.\nA small note: if you want to add relative links from a blog post to another post in your same blog. So [this](/post/2019-02-19-hugo-archetypes/) becomes this.\nNow, add images and data files to your ❤️’s content! But you may want to do one more thing…\n Update Metadata If you are anything like me, you may draft a blog post then come back to it later. For example, I started this post 2 days ago, but want to publish it today, 2019-10-03. The cool thing that was already built-in to blogdown is the “Update Metadata” Addin. With your blog post open (it should be called index.Rmd)1, click on Addins and select “Update Metadata”. You should see a window like this:\nCheck the box to rename the file if the date has changed. RStudio will tell you your file has been deleted- which is technically true since the folder was renamed, but don’t panic!\nClick YES. The index.Rmd file that is now open should have an updated date field in the YAML. In your RStudio file viewer, you may want to click on “content” at this point then navigate back to view your post- then you will then see that the folder name now has an updated date too.\n  If no post is open, you will get an error: Warning message: The current document does not seem to contain YAML metadata↩︎\n   ","date":1550707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550707200,"objectID":"57794aaa1dd7911236dd044032a33bf8","permalink":"https://nina-dl.github.io/post/2019-02-21-hugo-page-bundles/","publishdate":"2019-02-21T00:00:00Z","relpermalink":"/post/2019-02-21-hugo-page-bundles/","section":"post","summary":"Why (and how) you should use Hugo's new page bundles feature","tags":["blogdown"],"title":"A Spoonful of Hugo: Page Bundles","type":"post"},{"authors":["alison"],"categories":["blogdown","netlify","hugo"],"content":"  “Just a spoonful of Hugo helps the blog go down.” - me, only somewhat kidding\n You can read the previous post about my “Spoonful of Hugo” series here. In this series, I’m sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated).\nThis is my second post in this series, and it is a relatively quick one. Just do this. This one is a no-brainer.\n Thanks to Mara Averick for alerting me that with Hugo version 0.54.0 and onward, there is a trailing zero at the end of Hugo versions now. So for versions before 0.54.0, use the format: 0.53; for later versions use 0.54.0 (0.54 will not work).   Use Netlify to Deploy First, you’ll need to use Netlify! I am a very happy Netlify user and currently have approximately 33 sites deployed. To setup a new account, navigate to Netlify and click on the Sign Up link.\nSign up with GitHub to connect your GitHub and Netlify accounts (as shown below).\nIf you use a different version control service, select GitLab or BitBucket instead.\nThe last step is to use the Netlify UI in browser do New Site from Git \u0026gt; pick your repo. You’ll be prompted to fill in these fields, they are probably already filled in correctly for you:\nThe next part is the advanced build settings:\nSee that pro tip about the netlify.toml? Let’s do that! You can leave these fields as is.\n Why netlify.toml? In their Build Gotchas:\n “If your build works locally, the next debugging step is to ensure the package versions we use to build match yours. You can find the settings for these in the Build Settings doc. That’s the leading cause of build failure.”\n Yes that is right- package version mismatches are the leading cause of build failure with Netlify. What does this look like for blogdown users? This means that you are running a version of Hugo locally that doesn’t match the version that Netlify is using to build your site. Most of the time, you are using a more recent version of Hugo than the one Netlify uses. This means that the files your theme relies on may be using newer Hugo functions that were introduced in later Hugo versions- functions that Netlify won’t be able to find working from an older Hugo version. You’ll get all the build errors.\nYou can check your local Hugo version by running this code in your R console:\nblogdown::hugo_version() ## [1] \u0026#39;0.57.2\u0026#39; Now, we want Netlify to use this same version of Hugo when it builds your site. You can do this two ways:\nDo this in your browser (👎) Do this in your project root directory in a netlify.toml file (👍)   Add the netlify.toml File Adding this file means that team members can see for themselves what version of Hugo you are running- if it is buried in the Netlify UI, you can’t see that information unless you sift through the public build logs (no thanks). Making the file as plain text in the root of your blogdown project directory means that:\n it is version controlled (yay!) and other people who use/learn from/contribute to your blog can actually reproduce your site with the same site configuration. Bonus: you can set the Hugo versions for branch deploys too.  Here is an example from my own netlify.toml file1:\n[build] publish = \u0026quot;public\u0026quot; command = \u0026quot;hugo\u0026quot; [context.production.environment] HUGO_VERSION = \u0026quot;0.54.0\u0026quot; # if older, use format: 0.53 (no trailing zero) HUGO_ENV = \u0026quot;production\u0026quot; HUGO_ENABLEGITINFO = \u0026quot;true\u0026quot; [context.branch-deploy.environment] HUGO_VERSION = \u0026quot;0.54.0\u0026quot; # if older, use format: 0.53 (no trailing zero) [context.deploy-preview.environment] HUGO_VERSION = \u0026quot;0.54.0\u0026quot; You can leave off the last two chunk if you don’t want to use branch deploys or preview deploys, but I ❤️ these two Netlify features and encourage you to try them out. I’ve starting drafting individual blog posts and tutorials in branches, and then I can see them rendered and share them for feedback without asking collaborators to clone and build the repository locally. It is lovely. Every branch and pull request gets a link 🎉.\nSo add this file to your blogdown site repo and push to GitHub.\nNote that, according to the Netlify docs:\n “During a build, the following ordering determines which context covers a particular deploy: UI settings are overridden if a netlify.toml file is present in the root folder of the repo and there exists a setting for the same property/redirect/header in the toml file.”\n If you look in your site’s Netlify deploy log, you should see entries like this:\n7:47:13 PM: Found netlify.toml. Overriding site configuration 7:47:13 PM: Starting build script 7:47:13 PM: Installing dependencies 7:47:14 PM: Started restoring cached node version 7:47:17 PM: Finished restoring cached node version 7:47:18 PM: v8.15.0 is already installed. 7:47:19 PM: Now using node v8.15.0 (npm v6.4.1) 7:47:19 PM: Attempting ruby version 2.3.6, read from environment 7:47:20 PM: Using ruby version 2.3.6 7:47:20 PM: Using PHP version 5.6 7:47:20 PM: Installing Hugo 0.54.0 Success!\n  the leading zero matters for Hugo versions, so 0.53 works but .53 will not. For versions \u0026gt;= 0.54.0, the trailing zero also matters, so 0.54.0 works but 0.54 will not.↩\n   ","date":1550620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550620800,"objectID":"8a82d0bfce9f49b7c489323259c476b4","permalink":"https://nina-dl.github.io/post/2019-02-19-hugo-netlify-toml/","publishdate":"2019-02-20T00:00:00Z","relpermalink":"/post/2019-02-19-hugo-netlify-toml/","section":"post","summary":"Why you should use a netlify.toml file in your blogdown site","tags":["blogdown","netlify","hugo"],"title":"A Spoonful of Hugo: The netlify.toml File","type":"post"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"  “Just a spoonful of Hugo helps the blog go down.” - me, only somewhat kidding\n As a happy blogdown user, a common story I hear from other #rstats users is that you try to change one little thing in Hugo, and the whole site breaks. Here be dragons for folks who aren’t web developers.\nI’m here to tell you that there are small spoonfuls of Hugo that can help you get your site UP (and even better- more efficient, more streamlined, more automated), even if you are not in the least bit interested in transitioning into a career in web development 😏.\nMy Project The education team at RStudio needs a website and we have a short wishlist:\n We want something we can maintain ourselves, We want to look consistent with other RStudio sites on the outside, and We want to be consistent on the inside so that we can get help if/when we need it.  This led me to the current tidyverse.org blogdown site. I wanted to make a copy of the site then customize for the education team, but I noticed that the source code for the site didn’t make it easy for me to copy the structure of the site and edit only the content of the site. This is one of the real strengths of Hugo, so I embarked on a learning adventure.\n   via GIPHY  As a result, I have been living and breathing Hugo lately. As in, my husband now recognizes Mike Dane’s voice. You may not have have met Mike yet, but he appears in all the video tutorials in the Hugo docs. His screencasts have been really helpful to me, like this one on templating. I’ve also spent a lot of time actually reading the docs (which are pretty good!), reading posts and answers on the Hugo discourse community site, and spelunking around inside the actual source code for two very well structured Hugo sites:\nThe actual Hugo site: https://github.com/gohugoio/hugoDocs The rOpenSci site: https://github.com/ropensci/roweb2  I’ll be using this post and other later posts to share some of the things I’ve learned about Hugo along the way. Mainly breadcrumbs to myself, but I hope these help other people too.\nFor reference, I’m using Hugo via the blogdown R package, and within the RStudio IDE. These are my blogdown and Hugo versions:\npackageVersion(\u0026quot;blogdown\u0026quot;) ## [1] \u0026#39;0.12.1\u0026#39; blogdown::hugo_version() ## [1] \u0026#39;0.55.6\u0026#39;  tl;dr: A Teaspoon of Archetypes Add custom archetypes as .md files to your project root directory (do not touch the archetypes folder in your themes/archetypes folder).  If you don’t have that as an empty folder in your project root, make one, then add your archetype files to it. If you are making a new blogdown site, I recommend using these options to keep your empty directories1:  library(blogdown) new_site(theme = \u0026quot;jpescador/hugo-future-imperfect\u0026quot;, sample = TRUE, theme_example = TRUE, empty_dirs = TRUE, # this! to_yaml = TRUE)  Figure 1: Using the RStudio Project Wizard  Use the “New Post” Addin in RStudio to create any and all new content for your site (not just posts!). Be sure to use the handy dropdown menu to select from all the possible archetypes. Also, careful about the subdirectory here- some themes use blog, others use news, articles, or posts.\n Your archetypes, while only markdown files, can include R code. When you use the Addin, be sure to choose R Markdown (.Rmd) as the format so that you can run the code.  Don’t miss this great blog post by my friend and the great educator Leo Collado-Torres on archetypes.    A Tablespoon of Archetypes One of the easiest things you can do for yourself is customize your site’s archetypes. From the Hugo docs:\n “Archetypes are templates used when creating new content.”\n Right away when I cloned the tidyverse site, I noticed that there were instructions for how to contribute a new article (or blog post) in the README.md and in a separate CONTRIBUTING.md file. Then I noticed this open GitHub issue from Mara Averick (the tidyverse developer advocate) titled “Fix README/CONTRIBUTING so there’s one source of mechanical info?”.\nI also noticed that there was no project root folder called archetypes, which is where you would store your custom site archetype files as .md files. In fact, there is no theme folder as you might expect either, which is where you could view the default theme archetypes. Let’s look at some from other Hugo themes:\nThe default Hugo theme for blogdown, Lithium, has just one archetype: default.md\n--- title: \u0026#39;\u0026#39; date: \u0026#39;\u0026#39; --- In contrast, the Hugo Academic theme has A LOT: https://github.com/gcushen/hugo-academic/tree/master/archetypes; here is the content of the one for new posts:\n+++ title = \u0026quot;{{ replace .Name \u0026quot;-\u0026quot; \u0026quot; \u0026quot; | title }}\u0026quot; subtitle = \u0026quot;\u0026quot; # Add a summary to display on homepage (optional). summary = \u0026quot;\u0026quot; date = {{ .Date }} draft = false # Authors. Comma separated list, e.g. `[\u0026quot;Bob Smith\u0026quot;, \u0026quot;David Jones\u0026quot;]`. authors = [] # Tags and categories # For example, use `tags = []` for no tags, or the form `tags = [\u0026quot;A Tag\u0026quot;, \u0026quot;Another Tag\u0026quot;]` for one or more tags. tags = [] categories = [] # Projects (optional). # Associate this post with one or more of your projects. # Simply enter your project\u0026#39;s folder or file name without extension. # E.g. `projects = [\u0026quot;deep-learning\u0026quot;]` references # `content/project/deep-learning/index.md`. # Otherwise, set `projects = []`. # projects = [\u0026quot;internal-project\u0026quot;] # Featured image # To use, add an image named `featured.jpg/png` to your page\u0026#39;s folder. [image] # Caption (optional) caption = \u0026quot;\u0026quot; # Focal point (optional) # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight focal_point = \u0026quot;\u0026quot; +++  A quick note: you may have noticed differences in both the content between these two files but also the structure. The first is a YAML file, the second is a TOML file. For blogdown users, you may want to use YAML. This is also why I recommend when you set up your site to use the to_yaml = TRUE option (in the Project Wizard from figure 1, check the “Convert all metadata to YAML” box; otherwise, the exampleSite will contain TOML instead of YAML)2.\nIf you read the original tidyverse CONTRIBUTING.md file, the instructions include a fair bit of R code that I would guess means a lot of copying and pasting into new posts. For example, the R Markdown setup chunk and the code for using usethis::use_tidy_thanks() for package releases. I studied the contributing guidelines, and parsed three different “kinds” of articles that are commonly contributed, each with a different archetype:\nThe default.md- this is just for plain old markdown posts and basically sets up the YAML of the post to be the same as it is now (currently, there is no archetype dictating the content- it is pulling from a project-level .Rprofile).\n A default-rmarkdown.md which should only be used with an R Markdown post and provides only the setup chunk at the top.\n A package-release.md which also should only be used with an R Markdown post and adds the usethis::use_tidy_thanks() code chunk (this is pseudo-code so the default chunk option is set to eval = FALSE).\n  So I drafted a pull request that adds these three archetypes to the GitHub repository for the tidyverse.org. Here is the “after” Addin view:\nHere’s hoping Hugo archetypes make some things about adding new content to your site easier. There is no Hugo involved, other than realizing that Hugo will look first in your themes/\u0026lt;THEME-NAME\u0026gt;/archetypes/ folder, then in your project root archetypes/ folder next. DO NOT TOUCH any files in your themes/ directory.3\nYou may want to set up archetypes for your blogdown site if you have a “signature” R setup chunk that loads your preferred knitr chunk options, common libraries you always load at setup like tidyverse, ggplot2 themes you prefer (theme_minimal() FTW), etc. This may be especially helpful if you have multiple team members contributing to a single site and you want their posts to have a uniform setup. Then archetypes can be a real time- and sanity-saver. Get more ideas from Leo’s blog post on archetypes. You can also make directory based archetypes if you use Hugo page bundles, which is a topic of a future post.\n  These setup options are newish to the blogdown package: https://github.com/rstudio-education/arm-workshop-rsc2019/issues/8↩\n If you end up with TOML in your content files, run this R code: hugo_convert(to = \u0026quot;YAML\u0026quot;, unsafe = TRUE)↩\n Trust me on this one- if you ever want to update your site this will make that process way harder.↩\n   ","date":1550534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550534400,"objectID":"a160260aa45ac1699baf5f119b4c4e60","permalink":"https://nina-dl.github.io/post/2019-02-19-hugo-archetypes/","publishdate":"2019-02-19T00:00:00Z","relpermalink":"/post/2019-02-19-hugo-archetypes/","section":"post","summary":"Why you should use Hugo archetypes in your blogdown site","tags":["blogdown"],"title":"A Spoonful of Hugo: Archetypes","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://nina-dl.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://nina-dl.github.io/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and my experiences","tags":null,"title":"About me","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://nina-dl.github.io/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Get in touch","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://nina-dl.github.io/research/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"A little more about my research interests","tags":null,"title":"Research interest","type":"widget_page"},{"authors":["alison"],"categories":["readr","readxl","data import"],"content":"  A shorter version of this blog post now appears as an article vignette for the readxl package, thank you to Jenny Bryan for the invitation!   A problem I run up against a lot when working with other people’s data is having multiple header rows in the source data file. I like to use readr functions to read in rectangular data like .csv and .tsv files, but if you skip rows at import using the skip argument, you lose the header row as well, which usually has column names. The problem I often have is that the header row has column names that I want to keep, but I’d like to skip the second row (or more), which has some junk in it. Usually this row is some kind of data dictionary inserted between the row of column names and the actual data.\nIn this post, I’ll walk through a solution to this problem, using the readr package. You can also watch along in the video.\n  Warning!: I made a mistake when I said readr uses the first 100 rows of your data to predict column types- it uses the first 1000 rows.\n Download stickers.csv Being sticker rich This dataset is from an article published in PLOS ONE called “Being Sticker Rich: Numerical Context Influences Children’s Sharing Behavior”. In this study, children (ages 3–11) received a small (12, “sticker poor”) or large (30, “sticker rich”) number of stickers, and were then given the opportunity to share their windfall with either one or multiple anonymous recipients. This type of experimental design is a version of the Dictator Game.\nThe main research questions the authors explored were: do the number of available resources and/or the number of potential recipients alter the likelihood of a child donating and/or the amount they donate? But, in order to answer this question, we have to be able to read in the data! Luckily, these lovely developmental psychologists opted to share their data on the Harvard Dataverse as a tab-delimited file.\nIf you download the file, you can open it up in a plain text editor. You can also open it with Microsoft Excel.  Read in the file Let’s start by creating a variable called link to store the link to the data file.\n# create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; The file has a .tab extension, so we know it is tab-delimited. This means that the right readr function for reading this file is read_tsv. Since we stored our link already as a character string, that is the only argument to the read_tsv function.\n#install.packages(\u0026quot;readr\u0026quot;) library(readr) # load the readr package stickers \u0026lt;- read_tsv(link) # spec() Now, we know the second row of data is wonky, but how can we see that in R? There are a number of ways we can go spelunking around into our data file. The easiest to print it. Since we used readr, we have a tibble, which nicely prints to screen.\nstickers # # A tibble: 402 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 [Included Sa… 1=12:1; … 1=12; 2=30 1=1 recipient;… 1=fem… NA # 2 1 1 1 1 1 36 # 3 2 1 1 1 2 36 # 4 3 1 1 1 2 36 # 5 4 1 1 1 1 36 # 6 5 1 1 1 2 36 # 7 6 1 1 1 2 36 # 8 7 2 1 2 1 36 # 9 8 2 1 2 2 36 # 10 9 3 2 1 2 36 # # … with 392 more rows, and 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, # # Agegroups \u0026lt;chr\u0026gt;, `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, # # RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; Unfortunately, dplyr::glimpse can’t help us much, because we have one variable name that is ridiculously long (absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)). We’ll fix that with dplyr::rename.\nlibrary(dplyr) glimpse(stickers) # Observations: 402 # Variables: 18 # $ SubjectNumber \u0026lt;chr\u0026gt; … # $ Condition \u0026lt;chr\u0026gt; … # $ NumberStickers \u0026lt;chr\u0026gt; … # $ NumberEnvelopes \u0026lt;chr\u0026gt; … # $ Gender \u0026lt;chr\u0026gt; … # $ Agemonths \u0026lt;dbl\u0026gt; … # $ Ageyears \u0026lt;dbl\u0026gt; … # $ Agegroups \u0026lt;chr\u0026gt; … # $ `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt; … # $ LeftEnvelope \u0026lt;chr\u0026gt; … # $ RightEnvelope \u0026lt;chr\u0026gt; … # $ `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt; … # $ `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt; … # $ Giveornot \u0026lt;chr\u0026gt; … # $ LargerEnvelopeabs \u0026lt;chr\u0026gt; … # $ LargeEnvelopepercent \u0026lt;chr\u0026gt; … # $ SmallerEnvelopeabs \u0026lt;chr\u0026gt; … # $ SmallEnvelopepercent \u0026lt;chr\u0026gt; … More options:\nhead(stickers) # # A tibble: 6 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 [Included Sa… 1=12:1; … 1=12; 2=30 1=1 recipient;… 1=fem… NA # 2 1 1 1 1 1 36 # 3 2 1 1 1 2 36 # 4 3 1 1 1 2 36 # 5 4 1 1 1 1 36 # 6 5 1 1 1 2 36 # # … with 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;, # # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; tail(stickers) # # A tibble: 6 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 396 1 1 1 2 136 # 2 397 4 2 2 1 136 # 3 398 1 1 1 1 137 # 4 399 1 1 1 2 137 # 5 400 4 2 2 2 139 # 6 401 3 2 1 1 143 # # … with 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;, # # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; names(stickers) # [1] \u0026quot;SubjectNumber\u0026quot; # [2] \u0026quot;Condition\u0026quot; # [3] \u0026quot;NumberStickers\u0026quot; # [4] \u0026quot;NumberEnvelopes\u0026quot; # [5] \u0026quot;Gender\u0026quot; # [6] \u0026quot;Agemonths\u0026quot; # [7] \u0026quot;Ageyears\u0026quot; # [8] \u0026quot;Agegroups\u0026quot; # [9] \u0026quot;Subject\u0026#39;sEnvelope\u0026quot; # [10] \u0026quot;LeftEnvelope\u0026quot; # [11] \u0026quot;RightEnvelope\u0026quot; # [12] \u0026quot;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026quot; # [13] \u0026quot;PercentGiven(Outof100percent)\u0026quot; # [14] \u0026quot;Giveornot\u0026quot; # [15] \u0026quot;LargerEnvelopeabs\u0026quot; # [16] \u0026quot;LargeEnvelopepercent\u0026quot; # [17] \u0026quot;SmallerEnvelopeabs\u0026quot; # [18] \u0026quot;SmallEnvelopepercent\u0026quot; # View() Now we are ready to diagnose the problem!\nProblem: the first row is not really data. It is metadata about the variables, and it is screwing up readr’s ability to predict our column types.\nSolution: we’ll use readr and the read_tsv() function to read in the data twice. In Step 1, we’ll create a character vector of the column names only. In Step 2, we’ll read in the actual data and skip the multiple header rows at the top. When we do this, we lose the column names, so we use the character vector of column names we created in Step 1 instead.\n Read in the file (again) Step 1 Goal: we want to read in the first row only and save it as a character vector called sticker_names. This row contains the correct column names that we’ll need in Step 2.\nsticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() sticker_names # [1] \u0026quot;SubjectNumber\u0026quot; \u0026quot;Condition\u0026quot; # [3] \u0026quot;NumberStickers\u0026quot; \u0026quot;NumberEnvelopes\u0026quot; # [5] \u0026quot;Gender\u0026quot; \u0026quot;Agemonths\u0026quot; # [7] \u0026quot;Ageyears\u0026quot; \u0026quot;Agegroups\u0026quot; # [9] \u0026quot;Subject\u0026#39;sEnvelope\u0026quot; \u0026quot;LeftEnvelope\u0026quot; # [11] \u0026quot;RightEnvelope\u0026quot; \u0026quot;stickersgiven\u0026quot; # [13] \u0026quot;PercentGiven(Outof100percent)\u0026quot; \u0026quot;Giveornot\u0026quot; # [15] \u0026quot;LargerEnvelopeabs\u0026quot; \u0026quot;LargeEnvelopepercent\u0026quot; # [17] \u0026quot;SmallerEnvelopeabs\u0026quot; \u0026quot;SmallEnvelopepercent\u0026quot; glimpse(sticker_names) # chr [1:18] \u0026quot;SubjectNumber\u0026quot; \u0026quot;Condition\u0026quot; \u0026quot;NumberStickers\u0026quot; ...  Step 2 Goal: we want to read in all the rows except for the first two rows, which contained the variable names and variable descriptions. We want to save this as stickers, and set the column names to the sticker_names object we created in Step 1.\nstickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names) glimpse(stickers) # Observations: 401 # Variables: 18 # $ SubjectNumber \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1… # $ Condition \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3,… # $ NumberStickers \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,… # $ NumberEnvelopes \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,… # $ Gender \u0026lt;dbl\u0026gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2,… # $ Agemonths \u0026lt;dbl\u0026gt; 36, 36, 36, 36, 36, 36, 36, 36, … # $ Ageyears \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,… # $ Agegroups \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… # $ `Subject\u0026#39;sEnvelope` \u0026lt;dbl\u0026gt; 7, 12, 4, 7, 12, 8, 8, 11, 26, 3… # $ LeftEnvelope \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 2, 1, 4, 0, 18… # $ RightEnvelope \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA… # $ stickersgiven \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 4, 1, 4, 0, 18… # $ `PercentGiven(Outof100percent)` \u0026lt;dbl\u0026gt; 0.42, 0.00, 0.67, 0.42, 0.00, 0.… # $ Giveornot \u0026lt;dbl\u0026gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,… # $ LargerEnvelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 1, NA… # $ LargeEnvelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.500000… # $ SmallerEnvelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA… # $ SmallEnvelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.500000…   Fin! All together now: the final solution!\n# load packages library(readr) library(dplyr) # create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; # read in column names only sticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() # read in data, set column names stickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names)  Addendum For good measure, I would add a final step to everything above and use janitor::clean_names() to put all the variable names into snake case. So my final final solution is here:\n# load packages library(readr) library(dplyr) library(janitor) # create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; # read in column names only sticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() # read in data, set column names stickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names) %\u0026gt;% clean_names() stickers # # A tibble: 401 x 18 # subject_number condition number_stickers number_envelopes gender # \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; # 1 1 1 1 1 1 # 2 2 1 1 1 2 # 3 3 1 1 1 2 # 4 4 1 1 1 1 # 5 5 1 1 1 2 # 6 6 1 1 1 2 # 7 7 2 1 2 1 # 8 8 2 1 2 2 # 9 9 3 2 1 2 # 10 10 3 2 1 2 # # … with 391 more rows, and 13 more variables: agemonths \u0026lt;dbl\u0026gt;, # # ageyears \u0026lt;dbl\u0026gt;, agegroups \u0026lt;dbl\u0026gt;, subjects_envelope \u0026lt;dbl\u0026gt;, # # left_envelope \u0026lt;dbl\u0026gt;, right_envelope \u0026lt;dbl\u0026gt;, stickersgiven \u0026lt;dbl\u0026gt;, # # percent_given_outof100percent \u0026lt;dbl\u0026gt;, giveornot \u0026lt;dbl\u0026gt;, # # larger_envelopeabs \u0026lt;dbl\u0026gt;, large_envelopepercent \u0026lt;dbl\u0026gt;, # # smaller_envelopeabs \u0026lt;dbl\u0026gt;, small_envelopepercent \u0026lt;dbl\u0026gt; glimpse(stickers) # Observations: 401 # Variables: 18 # $ subject_number \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,… # $ condition \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3… # $ number_stickers \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2… # $ number_envelopes \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1… # $ gender \u0026lt;dbl\u0026gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1… # $ agemonths \u0026lt;dbl\u0026gt; 36, 36, 36, 36, 36, 36, 36, 36, 36… # $ ageyears \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… # $ agegroups \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… # $ subjects_envelope \u0026lt;dbl\u0026gt; 7, 12, 4, 7, 12, 8, 8, 11, 26, 30,… # $ left_envelope \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 2, 1, 4, 0, 18, … # $ right_envelope \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, … # $ stickersgiven \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 4, 1, 4, 0, 18, … # $ percent_given_outof100percent \u0026lt;dbl\u0026gt; 0.42, 0.00, 0.67, 0.42, 0.00, 0.33… # $ giveornot \u0026lt;dbl\u0026gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1… # $ larger_envelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 1, NA, … # $ large_envelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000,… # $ smaller_envelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, … # $ small_envelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000,…  Bonus data dictionary As an extra bonus, when you do have extra header rows, you can create a data dictionary using the gather() function from the tidyr package.\nlibrary(tidyr) stickers_dict \u0026lt;- read_tsv(link, n_max = 1) %\u0026gt;% rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% clean_names() %\u0026gt;% gather(variable_name, variable_description) stickers_dict # # A tibble: 18 x 2 # variable_name variable_description # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; # 1 subject_number [Included Sample Only] # 2 condition 1=12:1; 2=12:2, 3=30:1, 4=30:2 # 3 number_stickers 1=12; 2=30 # 4 number_envelopes 1=1 recipient; 2=2 recipients # 5 gender 1=female; 2=male # 6 agemonths \u0026lt;NA\u0026gt; # 7 ageyears \u0026lt;NA\u0026gt; # 8 agegroups 1=3-4yrs; 2=5-6yrs; 3=7-8yrs; 4=9-11yrs # 9 subjects_envelope How many stickers did the child keep for themse… # 10 left_envelope 1 recipient conditions: How many stickers the s… # 11 right_envelope 1 recipient conditions: N/A; 2 recipient condit… # 12 stickersgiven Regardless of condition, the number of stickers… # 13 percent_given_outof100… Regardless of condition, the proportion of stic… # 14 giveornot 1=Donated 1 or more stickers to the recipient(s… # 15 larger_envelopeabs Raw number of stickers (out of 30: Condition 2 … # 16 large_envelopepercent Proportion of stickers (out of 100%; Condition … # 17 smaller_envelopeabs Raw number of stickers (out of 30: Condition 2 … # 18 small_envelopepercent Proportion of stickers (out of 100%; Condition …  Useful resources  Great blog post from Lisa DeBruine using readxl to read in data with multiple header rows (including those with merged cells!): https://debruine.github.io/multirow_headers.html This GitHub issue with Hadley’s response that solved all my problems: https://github.com/tidyverse/readr/issues/179 My original tweet when I discovered this trick!  Neat #rstats #readr #tidyverse solution to read data when 1st row is header + 2nd row is junk, thanks @hadleywickham https://t.co/5TuH7vNaID pic.twitter.com/woZ3HuECge\n\u0026mdash; Alison Hill (@apreshill) September 4, 2017   ","date":1531008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531008000,"objectID":"c18d02feb96be2ee747ed7aa57c7bcc4","permalink":"https://nina-dl.github.io/post/2018-02-23-read-multiple-header-rows/","publishdate":"2018-07-08T00:00:00Z","relpermalink":"/post/2018-02-23-read-multiple-header-rows/","section":"post","summary":"Using the readr package to sidestep a common problem","tags":null,"title":"Read Data with Multiple Header Rows into R","type":"post"},{"authors":["alison"],"categories":["rladies","xaringan"],"content":" So, you are doing an R-Ladies presentation…that’s awesome!\nThe short version I made an R-Ladies theme for xaringan slides. My original tweet about it:\nif you want to use @xieyihui\u0026#39;s awesome #xaringan package for #rstats slides but want more #Rladies flavor, there is now a built-in theme for that (with code highlighting)! Thanks to the awesome @RLadiesGlobal starter kit. Update the CSS in your YAML to use 🧙🏽‍♀️🧞‍♀️ pic.twitter.com/YnlGSVAMsl\n\u0026mdash; Alison Hill (@apreshill) November 29, 2017  The way to use the theme is to update the YAML like so:\noutput: xaringan::moon_reader: css: [\u0026quot;default\u0026quot;, \u0026quot;rladies\u0026quot;, \u0026quot;rladies-fonts\u0026quot;] Make sure your version of xaringan is up-to-date.\nBelow is a demo slide deck using the theme.\n (view the source .Rmd on GitHub)\n The longer story I recommend Yihui’s xaringan package for slides. This is an R package, available through GitHub, for creating slideshows with remark.js through R Markdown. This means that you can:\n write all your slides in Markdown text include chunks of R code and rendered output like plots, results, tables, etc. in your slides use git for version control and share your GitHub repository  This makes xaringan ideal for an R-Ladies presentation!1\nTo use the package, you’ll need the devtools package installed so that you can use the install_github function. Then do:\ndevtools::install_github(\u0026#39;yihui/xaringan\u0026#39;) As Yihui points out in the documentation, if you use RStudio, you can use the menu to navigate to File -\u0026gt; New File -\u0026gt; R Markdown -\u0026gt; From Template -\u0026gt; Ninja Presentation, and you will see an R Markdown example.\nI first used xaringan a few months ago. I was working with Yihui on the blogdown book, and had signed up to lead a workshop for the Portland R User group. Obviously, such a workshop could not have powerpoint slides, so it seemed like the perfect time to learn xaringan.\nFor my workshop, I made a simple website for the newly founded R-Ladies PDX using blogdown (Thanks to Augustina and Deeksha, our fearless organizers). So naturally, my slides needed more purple.\nLuckily, the R-Ladies run a tight ship- they have a starter kit on GitHub that details all the pretty purples they like.\n About a month after I did the R-Ladies blogdown workshop, I saw this blog post by Yihui:\n First, I thought this was such a cool idea and I hope more people make and submit themes. Then I realized, I had already made a theme! I submitted a pull request2, Yihui helped me make some edits to the CSS files to make them more parsimonious with the default theme, I electronically signed a contributor agreement, and now the theme is there for you all to enjoy and use! You use the theme by editing the YAML:\noutput: xaringan::moon_reader: css: [\u0026quot;default\u0026quot;, \u0026quot;rladies\u0026quot;, \u0026quot;rladies-fonts\u0026quot;] If you use the theme and you are on twitter, I’d love to see it- please mention me on twitter!\nExamples!\n My blogdown workshop slides: “Up and running with blogdown” (view the source .Rmd on GitHub)    Jessica Minnier’s slides for “Building Shiny Apps: With Great Power Comes Great Responsibility”     If you are new to xaringan, don’t miss the wiki!↩\n Yihui’s technical instructions for contributors section of that blog post has been revised and is very detailed↩\n   ","date":1513555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513555200,"objectID":"4bae6dbc2d30b9c31fdd99c5dcdd6a81","permalink":"https://nina-dl.github.io/post/2017-12-18-r-ladies-presentation-ninja/","publishdate":"2017-12-18T00:00:00Z","relpermalink":"/post/2017-12-18-r-ladies-presentation-ninja/","section":"post","summary":"A guide to using the R-Ladies xaringan slide theme","tags":["xaringan"],"title":"R-Ladies Presentation Ninja","type":"post"},{"authors":["alison"],"categories":["blogdown","hugo","netlify"],"content":"  1 Read up on blogdown 2 Caveats, disclaimers, etc. 3 In GitHub 4 In terminal 5 In RStudio 6 Build your site in RStudio 7 Deploy in Netlify 8 Going further   1 Read up on blogdown Before you start, I recommend reading the following:\n blogdown: Creating Websites with R Markdown by Yihui Xie and Amber Thomas Making a Website Using blogdown, Hugo, and GitHub pages also by Amber Thomas  I also found this comment by Eric Nantz, the creator of the R-Podcast, in the rbind/support issues section on GitHub to be helpful:\n https://github.com/rbind/support/issues/12   2 Caveats, disclaimers, etc. Even with all the great resources I listed above, getting myself up and running took a few tries, so in this post I’m passing along what ended up working for me. Everyone’s mileage may vary, though, depending on your operating system and your approach. About me: I am a macOS user, and I use R, RStudio, Git (usually via GitLab, sometimes via GitHub), and terminal regularly, so I’m assuming familiarity here with all of these. If that is not you, here are some places to get started:\n For Git: Happy Git with R by Jenny Bryan et al. For RStudio: DataCamp’s Working with the RStudio IDE (free) by Garrett Grolemund For Terminal: The Command Line Murder Mystery by Noah Veltman, and The UNIX Workbench by Sean Kross  I also have Xcode and Homebrew installed- you will probably need these to download Hugo. If you don’t have either but are on a mac, this link may help:\n How to install Xcode, Homebrew, Git, RVM, Ruby \u0026amp; Rails on Mac OS X  Finally, I did not want to learn more about a lot of things! For instance, the nitty gritty of static site generators and how domain names work. I am a new mom, and just in the process of writing all this up, I filled up my tea mug twice with ice cold water, and filled my water bottle with scalding hot water. So, where offered, I followed the advice of Yihui and Amber. For example:\n “Considering the cost and friendliness to beginners, we currently recommend Netlify.” Sold. “If you are not familiar with domain names or do not want to learn more about them, an option for your consideration is a free subdomain *.rbind.io offered by RStudio, Inc.”. Done.   3 In GitHub  Go online to your GitHub account, and create a new repository (check to initialize with a README but don’t add .gitignore- this will be taken care of later). For naming your repo, consider your future deployment plan:\n If you are going to use Netlify to host the site, you can name this repository anything you want!  You can see some of the repo names used by members of the rbind organization here.    If you want to host your site as a GitHub Page, you should name your repository yourgithubusername.github.io (so mine would have been apreshill.github.io). If you are going this route, I suggest you follow Amber’s instructions instead of mine!   Screenshot above: Creating a new repository in GitHub\n Go to the main page of your new repository, and under the repository name, click the green Clone or download button.\n In the Clone with HTTPs section, click on the clipboard icon to copy the clone URL for your new repository. You’ll paste this text into terminal in the next section.\n   4 In terminal  Now you will clone your remote repository and create a local copy on your computer so you can sync between the two locations (using terminal or your alternative command line tool for a Windows machine).\nUse cd to navigate into the directory where you want your repo to be\n Once there, type: git clone [paste]. So my command looked like this:\n  git clone https://github.com/apreshill/apreshill.git And this is what printed to my terminal window:\nCloning into \u0026#39;apreshill\u0026#39;... remote: Counting objects: 3, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. Checking connectivity... done. Close terminal, you are done in there.   5 In RStudio  Install blogdown from your RStudio console. If you already have devtools installed like I did, you can just use the second line below:  if (!requireNamespace(\u0026quot;devtools\u0026quot;)) install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;rstudio/blogdown\u0026quot;) Install Hugo using the blogdown package helper function:  blogdown::install_hugo() # or library(blogdown) install_hugo()  This is where my instructions diverge from Ed’s- he states that blogdown won’t create a website in your root folder because the README.md file is already there. I didn’t find that to be the case- I tested this with a new site as well. If one way doesn’t work for you, try the other!   Use the top menu buttons in RStudio to select File -\u0026gt; New Project -\u0026gt; Existing Directory, then browse to the directory on your computer where your GitHub repo is and click on the Create Project button.  Screenshot above: Creating a new project in an existing directory in RStudio\n Now you should be “in” your project in RStudio. If you are using git for version control, edit your *gitignore file. This file should be viewable in your file viewer pane in RStudio. Below is what it should look like: the first four lines will automatically be in this file if you have set up your RStudio Project, but if you plan to use Netlify to deploy, you need to add the public/ line (read about here.)  .Rproj.user .Rhistory .RData .Ruserdata blogdown .DS_Store # if a windows user, Thumbs.db instead public/ # if using Netlify  6 Build your site in RStudio  Now you can finally build your site using the blogdown::new_site() function. But first you should at least think about themes…\n6.1 Picking a theme There are over 90 Hugo themes. So I went back to the blogdown book. Thankfully, Yihui and Amber offer “to save you some time, we list a few themes below that match our taste…”. Huzzah- I went with hugo-academic! Whatever theme you choose, you’ll need to pick one of 3 ways to make your new site:\nIf you are happy with the default theme, which is the lithium theme, you can use:  blogdown::new_site() # default theme is lithium If you want a theme other than the default, you can specify the theme at the same time as you call the new_site function:  # for example, create a new site with the academic theme blogdown::new_site(theme = \u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE) If instead you want to add the theme later (like I did, because I didn’t see the above example until it was too late!), you can do this:  library(blogdown) new_site() # default theme is lithium # need to stop serving so can use the console again install_theme(\u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE, update_config = TRUE)  Now is a good time to re-read about blogdown::serve_site() and how LiveReload works (and how it blocks your R console by default)   I recommend setting theme_example = TRUE- some themes won’t provide an example site, but the academic theme did and I found it helpful to see. You can always delete the example content.\n 6.2 Update project options In your project in RStudio, go to the top menu bar of RStudio and select Tools -\u0026gt; Project Options and update following Yihui and Amber’s instructions.\n 6.3 Edit your configurations Relevant reading:\n blogdown book chapter on configuration Additional detail from Amber You can also view my config.toml file  Now, edit the baseurl in your config.toml file. The URL should always end with a / trailing slash. At this point, you probably haven’t deployed your site yet, so to view it locally you can use the Serve Site add-in, or run the blogdown::serve_site function. Both of these baseurls worked for me when viewing locally:\nbaseurl = \u0026quot;https://example.com/\u0026quot; baseurl = \u0026quot;/\u0026quot;  Make sure that the baseurl = listed ends with a trailing slash /!   Go ahead and edit all the other elements in the config.toml file now as you please- this is how you personalize your site!\n 6.4 Addins \u0026amp; workflow Relevant reading:\n blogdown book chapter on the RStudio IDE  Addins: use them- you won’t need the blogdown library loaded in the console if you use the Addins. My workflow in RStudio at this point (again, just viewing locally because we haven’t deployed yet) works best like this:\nOpen the RStudio project for the site Use the Serve Site add-in (only once due to the magic of LiveReload) View site in the RStudio viewer pane, and open in a new browser window while I work Select existing files to edit using the file pane in RStudio After making changes, click the save button (don’t knit!)- the console will reload, the viewer pane will update, and if you hit refresh in the browser your local view will also be updated When happy with changes, add/commit/push changes to GitHub  Having blogdown::serve_site running locally with LiveReload is especially useful as you can immediately see if you have totally screwed up. For example, in editing my about.md file, this error popped up in my console after making a change and I was able to fix the error right away:\nStarted building sites ... ERROR 2017/06/08 16:22:34 failed to parse page metadata for home/about.md: (18, 6): missing comma Error: Error building site: Errors reading pages: Error: failed to parse page metadata for home/about.md: (18, 6): missing comma for about.md The above workflow is only for editing existing files or posts, but not for creating new posts. For that, read on…\n 6.5 Posting Relevant reading:\n blogdown book chapter on RStudio IDE blogdown book chapter on output formats: on .md versus .Rmd posts Additional detail from Amber on adding a blog post  Bottom line:\nUse the New Post addin. But, you need the console to do this, so you have to stop blogdown::serve_site by clicking on the red Stop button first. The Addin is a Shiny interface that runs this code in your console: blogdown:::new_post_addin(). So, your console needs to be unblocked for it to run. You also need to be “in” your RStudio project or it won’t work.\n6.5.1 Draft posts Relevant reading:\n blogdown book chapter on building a website for local preview  Whether you do a markdown or R Markdown post (see below), you should know that in the YAML front matter of your new file, you can add draft: TRUE and you will be able to preview your post using blogdown::serve_site(), but conveniently your post will not show up on your deployed site until you set it to false. Because this is a function built into Hugo, all posts (draft or not) will still end up in your GitHub repo though.\n 6.5.2 New markdown posts Pick one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: Markdown (recommended) Use the console to author a new .md post:  blogdown::new_post() blogdown::new_post(ext = \u0026#39;.md\u0026#39;) # md is the default! Here are the ?new_post arguments:\nnew_post(title, kind = \u0026quot;\u0026quot;, open = interactive(), author = getOption(\u0026quot;blogdown.author\u0026quot;), categories = NULL, tags = NULL, date = Sys.Date(), file = NULL, slug = NULL, title_case = getOption(\u0026quot;blogdown.title_case\u0026quot;), subdir = getOption(\u0026quot;blogdown.subdir\u0026quot;, \u0026quot;post\u0026quot;), ext = getOption(\u0026quot;blogdown.ext\u0026quot;, \u0026quot;.md\u0026quot;))  Remember to use the Serve Site addin again so that you can immediately view your changes with every save using LiveReload.    6.5.3 New R Markdown (.Rmd) posts Again, you have your choice of one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: R Markdown (.Rmd) (recommended) Use the console to author a new .Rmd post:  blogdown::new_post(ext = \u0026#39;.Rmd\u0026#39;) # md is the default! After you edit your .Rmd post, in addition to saving the changes in your .Rmd file, you must use blogdown::serve_site- this is how the output html file needs to be generated.\n Do not knit your .Rmd posts- use blogdown::serve_site instead. If you happen to hit the knit button, just Serve Site again to rewrite the .html file.   Ultimately, your YAML front matter looks something like this; note that some but not all features of rmarkdown::html_document are supported in blogdown:\n--- title: \u0026quot;My Awesome Post\u0026quot; author: \u0026quot;John Doe\u0026quot; date: \u0026quot;2017-02-14\u0026quot; output: blogdown::html_page: toc: true toc_depth: 1 number_sections: true fig_width: 6 ---  Remember to use the Serve Site addin again so that you can immediately view your changes with every save using LiveReload and your .html file is properly output.    6.5.4 Adding images to a post If you want to include an image that is not a figure created from an R chunk, the recommended method is to:\nAdd the image to your /static/img/ folder, then Reference the image using the relative file path as follows:  ![my-image](/img/my-image.png)    7 Deploy in Netlify  Deploying in Netlify through GitHub is smooth. Yihui and Amber give some beginner instructions, but Netlify is so easy, I recommend that you skip dragging your public folder in and instead automate the process through GitHub.\nWhen you are ready to deploy, commit your changes and push to GitHub, then go online to Netlify. Click on the Sign Up button and sign up using your existing GitHub account (no need to create another account) Log in, and select: New site from Git -\u0026gt; Continuous Deployment: GitHub. From there, Netlify will allow you to select from your existing GitHub repositories. You’ll pick the repo you’ve been working from with blogdown, then you’ll configure your build. This involves specifying two important things: the build command and the publish directory (this should be public).\n More about the build command from Netlify: “For Hugo hosting, hugo will build and deploy with the version 0.17 of hugo. You can specify a specific hugo release like this: hugo_0.15. Currently 0.13, 0.14, 0.15, 0.16, 0.17, 0.18 and 0.19 are supported. For version 0.20 and above, you’ll need to create a Build environment variable called HUGO_VERSION and set it to the version of your choice.” I opted for the former, and specified hugo_0.19.   You can check your hugo version in terminal using the command hugo version. This is what my output looked like, so I could run version 0.20 if I wanted to through Netlify, but I went with 0.19 and it works just fine.\n$ hugo version Hugo Static Site Generator v0.20.7 darwin/amd64 BuildDate: 2017-05-08T18:37:40-07:00 Screenshot above: Basic build settings in Netlify\n Netlify will deploy your site and assign you a random subdomain name of the form random-word-12345.netlify.com. Mine was particularly unfortunate, with the random word garbage-collector-janice. You should know that you can change this; I changed mine to apreshill.netlify.com.\n Anytime you change your subdomain name, you need to update the baseurl in your config.toml file (so I changed mine to baseurl = “https://apreshill.netlify.com/”).   At this point, you should be up and running with blogdown, GitHub, and Netlify, but here are some ideas if you want to go further…\n 8 Going further 8.1 Custom CSS I like to tinker with default theme settings like colors and fonts. Every Hugo theme is structured a little differently, but if you are interested, you can check out my custom css to see how I customized the academic theme, which provides a way to link to a custom CSS file in the config.toml file:\n # Link custom CSS and JS assets # (relative to /static/css and /static/js respectively) custom_css = [\u0026quot;blue.css\u0026quot;]  8.2 Formspree I used Formspree to make a contact form, which is an online service (managed on GitHub) that allows you to add an HTML form to your static site. No registration, just use the form and confirm your email address once. I added the following code into my contact widget:\n\u0026lt;form action=\u0026quot;https://formspree.io/your@email.com\u0026quot; method=\u0026quot;POST\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;name\u0026quot;\u0026gt;Your name: \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;name\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026quot;email\u0026quot;\u0026gt;Your email: \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;email\u0026quot; name=\u0026quot;_replyto\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026quot;message\u0026quot;\u0026gt;Your message:\u0026lt;/label\u0026gt;\u0026lt;br\u0026gt; \u0026lt;textarea rows=\u0026quot;4\u0026quot; name=\u0026quot;message\u0026quot; id=\u0026quot;message\u0026quot; required=\u0026quot;required\u0026quot; class=\u0026quot;form-control\u0026quot; placeholder=\u0026quot;I can\u0026#39;t wait to read this!\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_next\u0026quot; value=\u0026quot;/html/thanks.html\u0026quot; /\u0026gt; \u0026lt;input type=\u0026quot;submit\u0026quot; value=\u0026quot;Send\u0026quot; name=\u0026quot;submit\u0026quot; class=\u0026quot;btn btn-primary btn-outline\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_subject\u0026quot; value=\u0026quot;Website message\u0026quot; /\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;_gotcha\u0026quot; style=\u0026quot;display:none\u0026quot; /\u0026gt; \u0026lt;/form\u0026gt;  8.3 *.rbind.io domain names You may want a different domain name than the one provided by Netlify. I opted for a free subdomain *.rbind.io offered by RStudio. To do the same, head over to the rbind/support GitHub page and open a new issue. All you need to do is let them know what your Netlify subdomain name is (*.netlify.com), and what you want your subdomain name to be (*.rbind.io). The awesome rbind support team will help you take it from there!\n Again, you will need to update the baseurl in your config.toml file to reflect your new rbind subdomain name (so mine is baseurl = “https://alison.rbind.io/”).     8.4 Have fun! Lastly, don’t forget to just have fun with it. Happy blogdowning!\n  via GIPHY   ","date":1497225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497225600,"objectID":"dbf96d61412119467fb5db386d4f4407","permalink":"https://nina-dl.github.io/post/2017-06-12-up-and-running-with-blogdown/","publishdate":"2017-06-12T00:00:00Z","relpermalink":"/post/2017-06-12-up-and-running-with-blogdown/","section":"post","summary":"A guide to getting up and running with blogdown, GitHub, and Netlify","tags":["blogdown"],"title":"Up \u0026 Running with blogdown","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"3e5f5f635da5c7c33683baadcc1a0c46","permalink":"https://nina-dl.github.io/project/miscellanea/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/miscellanea/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"85d4f9c847043100093969f1ad6109a0","permalink":"https://nina-dl.github.io/project/applications/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/applications/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]